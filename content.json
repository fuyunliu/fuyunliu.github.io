{"meta":{"title":"听听那冷雨","subtitle":"No one write here","description":"记录一点东西","author":"听听那冷雨","url":"https://fuyunliu.github.io","root":"/"},"pages":[{"title":"分类","date":"2018-03-10T09:46:57.000Z","updated":"2020-12-03T07:34:04.041Z","comments":true,"path":"categories/index.html","permalink":"https://fuyunliu.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-03-10T09:46:27.000Z","updated":"2020-12-03T07:33:48.333Z","comments":true,"path":"tags/index.html","permalink":"https://fuyunliu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"基于 LSTM 深度学习模型的日志异常检测","slug":"log-anomaly-detection","date":"2021-06-08T08:33:13.000Z","updated":"2021-06-08T08:39:02.318Z","comments":true,"path":"2021/06/08/log-anomaly-detection/","link":"","permalink":"https://fuyunliu.github.io/2021/06/08/log-anomaly-detection/","excerpt":"日志模式识别日志样例123456789102021-03-11 09:23:17,195 [OCKey:HlTOgWeM-LYFkTMn2wsXcEd] DEBUG com.dawninfotek.base.security.WebAppSafeGuard - BaseWebAppSafeGuard.class::key = beneAcctNo4 value = 12021-03-11 09:23:17,849 [OCKey:zQXeCzqsWdY9_GliHIoAIpI] DEBUG com.dawninfotek.base.action.BaseDispatchAction - com.dawninfotek.easybanking.web.pr.olbfinance.polbfinancemyfinancenew.POLBFinanceMyFinanceAction::Base BaseDispatchAction - link6::370037472021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG com.dawninfotek.base.dac.ConnectionFactory - java.lang.Class::Getting a connection from dataSource2021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG com.dawninfotek.base.dac.ConnectionFactory - java.lang.Class::got connection com.ibm.ws.rsadapter.jdbc.WSJdbcConnection@3bfa1e18 from dataSource: auto Commit = false2021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.Connection - &#123;conn-12085845&#125; Connection2021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.Connection - &#123;conn-12085845&#125; Preparing Statement: select count(T.CUSTNO) from PERCRMPRODUCTDUEDATE t where t.CUSTNO=? and t.READFLAG = &#x27;0&#x27;2021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.PreparedStatement - &#123;pstm-12085846&#125; Executing Statement: select count(T.CUSTNO) from PERCRMPRODUCTDUEDATE t where t.CUSTNO=? and t.READFLAG = &#x27;0&#x27;2021-03-11 09:23:18,636 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.PreparedStatement - &#123;pstm-12085846&#125; Parameters: [42300205]2021-03-11 09:23:18,636 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.PreparedStatement - &#123;pstm-12085846&#125; Types: [java.lang.String]2021-03-11 09:23:18,636 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG com.dawninfotek.easybanking.base.process.EasyBankingProcessManager - com.dawninfotek.easybanking.base.process.EasyBankingProcessManager::Start to commit transaction for DAC:EasyBankingSample","text":"日志模式识别日志样例123456789102021-03-11 09:23:17,195 [OCKey:HlTOgWeM-LYFkTMn2wsXcEd] DEBUG com.dawninfotek.base.security.WebAppSafeGuard - BaseWebAppSafeGuard.class::key = beneAcctNo4 value = 12021-03-11 09:23:17,849 [OCKey:zQXeCzqsWdY9_GliHIoAIpI] DEBUG com.dawninfotek.base.action.BaseDispatchAction - com.dawninfotek.easybanking.web.pr.olbfinance.polbfinancemyfinancenew.POLBFinanceMyFinanceAction::Base BaseDispatchAction - link6::370037472021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG com.dawninfotek.base.dac.ConnectionFactory - java.lang.Class::Getting a connection from dataSource2021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG com.dawninfotek.base.dac.ConnectionFactory - java.lang.Class::got connection com.ibm.ws.rsadapter.jdbc.WSJdbcConnection@3bfa1e18 from dataSource: auto Commit = false2021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.Connection - &#123;conn-12085845&#125; Connection2021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.Connection - &#123;conn-12085845&#125; Preparing Statement: select count(T.CUSTNO) from PERCRMPRODUCTDUEDATE t where t.CUSTNO=? and t.READFLAG = &#x27;0&#x27;2021-03-11 09:23:18,635 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.PreparedStatement - &#123;pstm-12085846&#125; Executing Statement: select count(T.CUSTNO) from PERCRMPRODUCTDUEDATE t where t.CUSTNO=? and t.READFLAG = &#x27;0&#x27;2021-03-11 09:23:18,636 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.PreparedStatement - &#123;pstm-12085846&#125; Parameters: [42300205]2021-03-11 09:23:18,636 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG java.sql.PreparedStatement - &#123;pstm-12085846&#125; Types: [java.lang.String]2021-03-11 09:23:18,636 [OCKey:cSrKSPklI9-juH5TB1VdT6N] DEBUG com.dawninfotek.easybanking.base.process.EasyBankingProcessManager - com.dawninfotek.easybanking.base.process.EasyBankingProcessManager::Start to commit transaction for DAC:EasyBankingSample 日志模式使用 Drain3 算法对日志进行模式提取，共提取得到 190 条日志模式，只展示部分结果。 12345678910&#123;&quot;group_id&quot;: 1, &quot;cluster_id&quot;: 1, &quot;cluster_size&quot;: 62086, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;MODULE&gt; key &lt;*&gt; value&quot;, &quot;template_tokenize&quot;: &quot;modul key valu&quot;&#125;&#123;&quot;group_id&quot;: 2, &quot;cluster_id&quot;: 2, &quot;cluster_size&quot;: 22856, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;CLASS&gt; key &lt;KEY&gt; value &lt;VALUE&gt;&quot;, &quot;template_tokenize&quot;: &quot;class key key valu valu&quot;&#125;&#123;&quot;group_id&quot;: 3, &quot;cluster_id&quot;: 3, &quot;cluster_size&quot;: 2942, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;MODULE&gt; Base BaseDispatchAction &lt;*&gt; &lt;*&gt;&quot;, &quot;template_tokenize&quot;: &quot;modul base basedispatchact&quot;&#125;&#123;&quot;group_id&quot;: 3, &quot;cluster_id&quot;: 53, &quot;cluster_size&quot;: 147, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;MODULE&gt; Base BaseDispatchAction &lt;*&gt;&quot;, &quot;template_tokenize&quot;: &quot;modul base basedispatchact&quot;&#125;&#123;&quot;group_id&quot;: 4, &quot;cluster_id&quot;: 4, &quot;cluster_size&quot;: 16820, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;MODULE&gt; Getting a connection from dataSource&quot;, &quot;template_tokenize&quot;: &quot;modul connect datasourc&quot;&#125;&#123;&quot;group_id&quot;: 5, &quot;cluster_id&quot;: 5, &quot;cluster_size&quot;: 16820, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;MODULE&gt; got connection com.ibm.ws.rsadapter.jdbc.WSJdbcConnection &lt;*&gt; from dataSource auto Commit false&quot;, &quot;template_tokenize&quot;: &quot;modul connect ibm rsadapt jdbc wsjdbcconnect datasourc auto commit fals&quot;&#125;&#123;&quot;group_id&quot;: 6, &quot;cluster_id&quot;: 6, &quot;cluster_size&quot;: 23956, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;CONN&gt; Connection&quot;, &quot;template_tokenize&quot;: &quot;conn connect&quot;&#125;&#123;&quot;group_id&quot;: 7, &quot;cluster_id&quot;: 7, &quot;cluster_size&quot;: 22323, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;CONN&gt; Preparing Statement &lt;SQL&gt;&quot;, &quot;template_tokenize&quot;: &quot;conn prepar statement sql&quot;&#125;&#123;&quot;group_id&quot;: 8, &quot;cluster_id&quot;: 8, &quot;cluster_size&quot;: 22323, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;PSTM&gt; Executing Statement &lt;SQL&gt;&quot;, &quot;template_tokenize&quot;: &quot;pstm execut statement sql&quot;&#125;&#123;&quot;group_id&quot;: 9, &quot;cluster_id&quot;: 9, &quot;cluster_size&quot;: 22322, &quot;is_abnormal&quot;: false, &quot;template_mined&quot;: &quot;&lt;PSTM&gt; Parameters &lt;PARAMS&gt;&quot;, &quot;template_tokenize&quot;: &quot;pstm paramet param&quot;&#125; 字段解释 123456group_id: 模式分组 ID，使用 Levenshtein 计算相似度，阈值为 0.8，大于等于阈值的模式会被分配到相同 group_id 之下。cluster_id：模式 ID。cluster_size: 模式匹配到的日志数量。is_abnormal: 根据通用的异常关键字，预判模式的异常。template_mined: 学习到的日志模式字符串。template_tokenize: 日志模式字符串经过一系列处理得到的字符串，后续日志模式向量计算基于该字符串。 预置的通用异常关键字 1abnormal_chars = [&#x27;error&#x27;, &#x27;fail&#x27;, &#x27;failed&#x27;, &#x27;exception&#x27;, &#x27;invalid&#x27;, &#x27;missing&#x27;, &#x27;duplicate&#x27;, &#x27;unable&#x27;] 日志模式字符串处理过程 提取中英文单词12345678910111213141516171819202122232425262728293031323334def is_lower_alphabet(char): &quot;&quot;&quot;小写字母&quot;&quot;&quot; return &#x27;\\u0061&#x27; &lt;= char &lt;= &#x27;\\u007A&#x27;def is_upper_alphabet(char): &quot;&quot;&quot;大写字母&quot;&quot;&quot; return &#x27;\\u0041&#x27; &lt;= char &lt;= &#x27;\\u005A&#x27;def is_alphabet(char): &quot;&quot;&quot;英文字母&quot;&quot;&quot; return is_lower_alphabet(char) or is_upper_alphabet(char)def is_chinese(char): &quot;&quot;&quot;中文&quot;&quot;&quot; return &#x27;\\u4e00&#x27; &lt;= char &lt;= &#x27;\\u9fff&#x27;def segment(text): &quot;&quot;&quot;中英文分词&quot;&quot;&quot; chars = [] for char in text: if not is_alphabet(char) and not is_chinese(char):: if chars: word = &#x27;&#x27;.join(chars) yield word chars = [] else: chars.append(char) if chars: word = &#x27;&#x27;.join(chars) yield word 中文分词如果提取得到的单词是中文，进一步将中文进行分词 1234import hanlpHanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)tokens = HanLP(&#x27;读取限流控制参数值&#x27;, tasks=&#x27;tok/fine&#x27;)[&#x27;tok/fine&#x27;] # [&#x27;读取&#x27;, &#x27;限流&#x27;, &#x27;控制&#x27;, &#x27;参数值&#x27;] 统一转化为小写1word = str.lower(word) 词干提取1234from nltk.stem.snowball import SnowballStemmerstemmer= SnowballStemmer(&#x27;english&#x27;)word = stemmer.stem(&#x27;Connection&#x27;) # connect 词形还原12345from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer()word = lemmatizer.lemmatize(&#x27;rooms&#x27;) # room 去停用词12345678910111213# 英文停用词: https://github.com/stopwords-iso/stopwords-en# 中文停用词: https://github.com/goto456/stopwordsfrom pathlib import Pathdef load_stopwords(): english_path = Path(__file__).parent / &#x27;stopwords&#x27; / &#x27;english&#x27; chinese_path = Path(__file__).parent / &#x27;stopwords&#x27; / &#x27;chinese&#x27; with english_path.open(&#x27;rt&#x27;, encoding=&#x27;utf-8&#x27;) as en, chinese_path.open(&#x27;rt&#x27;, encoding=&#x27;utf-8&#x27;) as zh: stopwords = &#123;line.strip() for line in en&#125; | &#123;line.strip() for line in zh&#125; return stopwords 模式向量计算TF-IDF 计算1234567891011121314from sklearn.feature_extraction.text import TfidfVectorizer# 读取日志模版template = pd.read_csv(&#x27;easybanking.csv&#x27;)template.fillna(&#x27;&#x27;, inplace=True)# TF-IDF 计算corpus = template[&#x27;template_tokenize&#x27;].to_list()vectorizer = TfidfVectorizer()X = vectorizer.fit_transform(corpus)xarr = X.toarray()words = vectorizer.get_feature_names() 词向量计算1234567891011121314151617import fasttextfasttext.FastText.eprint = lambda x: Noneften = fasttext.load_model(&#x27;cc.en.300.bin&#x27;)ftzh = fasttext.load_model(&#x27;cc.zh.300.bin&#x27;)class WordVectorDict(dict): def __missing__(self, key): if any(map(is_chinese, key)): return ftzh.get_word_vector(key) else: return ften.get_word_vector(key)word_vectors = WordVectorDict() 模式向量1234567891011121314# 模式向量计算template_vectors = &#123;&#125;for row, arr in zip(template.to_dict(orient=&#x27;records&#x27;), xarr): lookup = defaultdict(int, zip(words, arr)) tokens = row[&#x27;template_tokenize&#x27;].split() tv = np.zeros(300) for token in tokens: w = lookup[token] # TF-IDF v = word_vectors[token] # 词向量 wv = w * v tv += wv tv = tv if len(tokens) == 0 else tv / len(tokens) template_vectors[row[&#x27;cluster_id&#x27;]] = tv 模式相似度1234from sklearn.metrics.pairwise import cosine_similaritycs = cosine_similarity(template_vector_1.reshape(1,-1), template_vector_2.reshape(1,-1)) \b评估指标 TP：True Positive，被判定为正样本，事实上也是证样本。 TN：True Negative，被判定为负样本，事实上也是负样本。 FP：False Positive，被判定为正样本，但事实上是负样本。 FN：False Negative，被判定为负样本，但事实上是正样本。 precesion：查准率，precesion = TP/(TP+FP)。 recall：查全率，recall = TP/(TP+FN)。 基于模式概率分布的异常检测在没有得到日志模版向量表示之前，使用模版的索引对所有日志模版进行 One-Hot 编码，所以对于每一行日志都有一个其对应的模版编码，也可以叫模版概率分布。 对原始日志处理成模版的概率分布之后，我们可以训练一个多分类模型，预测下一个日志的模版概率分布，如果真实日志的模版不在预测的 topK 中，则认为日志是异常的。 构造数据集12345678910111213141516171819202122232425262728import numpy as npimport pandas as pdfrom sklearn.preprocessing import label_binarizefrom sklearn.model_selection import train_test_splitdef create_window_dataset(log_parsed_path, template_path, window_size, split_ratio=0.8): log_parsed = pd.read_csv(log_parsed_path) templates = pd.read_csv(template_path)[&#x27;cluster_id&#x27;].to_list() def fn(dataset, look_back, look_interval=0, look_forward=1): x, y = [], [] for i in range(len(dataset)-look_back-look_interval-look_forward): back = dataset[i:i+look_back,] forward = dataset[i+look_back+look_interval:i+look_back+look_interval+look_forward,] back_x = label_binarize(back, classes=templates) forward_y = label_binarize(forward, classes=templates)[0] x.append(back_x) y.append(forward_y) return np.array(x), np.array(y) dataset = log_parsed[[&#x27;cluster_id&#x27;]].values x, y = fn(dataset, window_size) x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=split_ratio, shuffle=False, stratify=None) return (x_train, y_train), (x_test, y_test) 模型定义123456789101112131415from tensorflow.keras import layers, models, optimizersdef build_model(input_shape, num_classes): inputs = layers.Input(shape=input_shape) x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(inputs) x = layers.Bidirectional(layers.LSTM(64))(x) outputs = layers.Dense(num_classes, activation=&#x27;softmax&#x27;)(x) model = models.Model(inputs=inputs, outputs=outputs) model.compile(loss=&#x27;categorical_crossentropy&#x27;, optimizer=optimizers.Adam(), metrics=[&#x27;accuracy&#x27;]) return model 模型训练12345678910111213141516171819202122232425262728293031# 构造数据集n_candidates = 10window_size = 10log_parsed_path = &#x27;easybanking_parsed_1w.csv&#x27;template_path = &#x27;easybanking_template.csv&#x27;(x_train, y_train), (x_test, y_test) = create_window_dataset(log_parsed_path, template_path, window_size)# 定义模型num_classes = y_train.shape[1]input_shape = (x_train.shape[1], x_train.shape[2])model = build_model(input_shape, num_classes)model.summary()# 模型训练call_backs = [ callbacks.ModelCheckpoint(&quot;best_model.h5&quot;, save_best_only=True, monitor=&quot;val_loss&quot;), callbacks.ReduceLROnPlateau(monitor=&quot;val_loss&quot;, factor=0.5, patience=20, min_lr=0.0001), callbacks.EarlyStopping(monitor=&quot;val_loss&quot;, patience=50, verbose=1),]history = model.fit(x_train, y_train, batch_size=32, epochs=20, callbacks=call_backs, validation_split=0.2, shuffle=True, verbose=1)# 评估模型best_model = models.load_model(&quot;best_model.h5&quot;)y_pred = best_model.predict(x_test)y_true = np.argmax(y_test, axis=1)y_pred = np.argmax(y_pred, axis=1)# 我们只知道日志模式预测的正确与否，但是不知道真实这条日志是否为异常，所以无法计算 TP、TN、FP 和 FN。# 简单起见计算全局准确率和召回率，这里只取 topK K = 1。真正需要提高准确率等需要在模型参数和日志数据及其处理等方面调节。precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=&#x27;micro&#x27;)# (0.7737737737737738, 0.7737737737737738, 0.7737737737737739, None) 缺点 该方案对日志模板进行编码时只使用模板索引，这可能导致丢失有价值的信息，因为模板索引不能揭示日志的语义关系。 真实系统不断升级和更新，日志的模式可能会相应地漂移，该方案难以应付不断变化的、有噪声的日志数据。 基于有监督二分类的异常检测我们得到了日志模版的向量表示，如果我们的数据集被人工标注了异常，我们很容易训练一个二分类模型，直接输出异常与否，由于应用日志没有异常标注，所以此方案做不了，为了实验可以使用公开的 HDFS 日志，这份日志有异常标注。 构造数据集1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def create_label_dataset(log_parsed_path, template_path, split_ratio=0.8): import fasttext fasttext.FastText.eprint = lambda x: None from sklearn.feature_extraction.text import TfidfVectorizer ften = fasttext.load_model(&#x27;/Users/fuyun/nltk_data/cc.en.300.bin&#x27;) ftzh = fasttext.load_model(&#x27;/Users/fuyun/nltk_data/cc.zh.300.bin&#x27;) class WordVectorDict(dict): def __missing__(self, key): if any(map(is_chinese, key)): return ftzh.get_word_vector(key) else: return ften.get_word_vector(key) # 读取日志模版 template = pd.read_csv(template_path) template.fillna(&#x27;&#x27;, inplace=True) # TF-IDF 计算 corpus = template[&#x27;template_tokenize&#x27;].to_list() vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(corpus) xarr = X.toarray() # 词向量计算 words = vectorizer.get_feature_names() word_vectors = WordVectorDict() # 模版向量计算 template_vectors = &#123;&#125; for row, arr in zip(template.to_dict(orient=&#x27;records&#x27;), xarr): lookup = defaultdict(int, zip(words, arr)) tokens = row[&#x27;template_tokenize&#x27;].split() tv = np.zeros(300) for token in tokens: w = lookup[token] # TF-IDF v = word_vectors[token] # 词向量 wv = w * v tv += wv tv = tv if len(tokens) == 0 else tv / len(tokens) template_vectors[row[&#x27;cluster_id&#x27;]] = tv # 读取日志解析结果 log_vectors = [] log_parsed = pd.read_csv(log_parsed_path) for cluster_id in log_parsed[&#x27;cluster_id&#x27;].to_list(): tv = template_vectors[cluster_id] log_vectors.append(tv) log_labels = log_parsed[&#x27;label&#x27;].to_list() x = np.array(log_vectors) x = np.reshape(x, (x.shape[0], 1, x.shape[1])) y = np.array(log_labels) x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=split_ratio, shuffle=False, stratify=None) # np.savez(&#x27;HDFS_10w.npz&#x27;, x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test) return (x_train, y_train), (x_test, y_test) 模型定义123456789101112131415from tensorflow.keras import layers, models, optimizersdef build_model(input_shape, num_classes): inputs = layers.Input(shape=input_shape) x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(inputs) x = layers.Bidirectional(layers.LSTM(64))(x) outputs = layers.Dense(num_classes, activation=&#x27;sigmoid&#x27;)(x) model = models.Model(inputs=inputs, outputs=outputs) model.compile(loss=&#x27;binary_crossentropy&#x27;, optimizer=optimizers.Adam(), metrics=[&#x27;accuracy&#x27;]) return model 模型训练1234567891011121314151617181920212223242526272829303132333435363738# 构造数据集log_parsed_path = &#x27;HDFS_parsed_10w.csv&#x27;template_path = &#x27;HDFS_template.csv&#x27;(x_train, y_train), (x_test, y_test) = create_label_dataset(log_parsed_path, template_path)# 定义模型num_classes = 1input_shape = (x_train.shape[1], x_train.shape[2])model = build_model(input_shape, num_classes)model.summary()# 模型训练call_backs = [ callbacks.ModelCheckpoint(&quot;best_model.h5&quot;, save_best_only=True, monitor=&quot;val_loss&quot;), callbacks.ReduceLROnPlateau(monitor=&quot;val_loss&quot;, factor=0.5, patience=20, min_lr=0.0001), callbacks.EarlyStopping(monitor=&quot;val_loss&quot;, patience=50, verbose=1),]history = model.fit(x_train, y_train, batch_size=32, epochs=20, callbacks=call_backs, validation_split=0.2, shuffle=True, verbose=1)# 评估模型best_model = models.load_model(&quot;best_model.h5&quot;)y_pred = best_model.predict(x_test)y_pred = y_pred[:,0].astype(int)precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=&#x27;binary&#x27;)# (0.9553976053045843, 1.0, 0.9771901149032715, None)# 画图metric = &quot;accuracy&quot;plt.figure()plt.plot(history.history[metric])plt.plot(history.history[&quot;val_&quot; + metric])plt.title(&quot;model &quot; + metric)plt.ylabel(metric, fontsize=&quot;large&quot;)plt.xlabel(&quot;epoch&quot;, fontsize=&quot;large&quot;)plt.legend([&quot;train&quot;, &quot;val&quot;], loc=&quot;best&quot;)plt.show()plt.close() 基于预测模式向量的异常检测由于没有异常标注数据，所以需要换一种思路进行异常检测，借鉴对时间序列的预测，我们可以认为日志是一种时间序列，此方案中我们训练的模型不是分类模型，而是回归模型，预测下一个日志的模版向量表示，与真实日志的模版向量进行相似度计算，计算出的相似度低于某个阈值则认为该日志是异常的，相似度采用余弦相似度。 构造数据集1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768def create_vector_dataset(log_parsed_path, template_path, window_size, split_ratio=0.8): import fasttext fasttext.FastText.eprint = lambda x: None from sklearn.feature_extraction.text import TfidfVectorizer ften = fasttext.load_model(&#x27;cc.en.300.bin&#x27;) ftzh = fasttext.load_model(&#x27;cc.zh.300.bin&#x27;) class WordVectorDict(dict): def __missing__(self, key): if any(map(is_chinese, key)): return ftzh.get_word_vector(key) else: return ften.get_word_vector(key) # 读取日志模版 template = pd.read_csv(template_path) template.fillna(&#x27;&#x27;, inplace=True) # TF-IDF 计算 corpus = template[&#x27;template_tokenize&#x27;].to_list() vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(corpus) xarr = X.toarray() # 词向量计算 words = vectorizer.get_feature_names() word_vectors = WordVectorDict() # 模版向量计算 template_vectors = &#123;&#125; for row, arr in zip(template.to_dict(orient=&#x27;records&#x27;), xarr): lookup = defaultdict(int, zip(words, arr)) tokens = row[&#x27;template_tokenize&#x27;].split() tv = np.zeros(300) for token in tokens: w = lookup[token] # TF-IDF v = word_vectors[token] # 词向量 wv = w * v tv += wv tv = tv if len(tokens) == 0 else tv / len(tokens) template_vectors[row[&#x27;cluster_id&#x27;]] = tv # 读取日志解析结果 log_vectors = [] log_parsed = pd.read_csv(log_parsed_path) for cluster_id in log_parsed[&#x27;cluster_id&#x27;].to_list(): tv = template_vectors[cluster_id] log_vectors.append(tv) def fn(dataset, look_back, look_interval=0, look_forward=1): x, y = [], [] for i in range(len(dataset)-look_back-look_interval-look_forward): back = dataset[i:i+look_back,] forward = dataset[i+look_back+look_interval:i+look_back+look_interval+look_forward,] x.append(back) y.append(forward) return np.array(x), np.array(y) x, y = fn(np.array(log_vectors), window_size) y = y[:,0] x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=split_ratio, shuffle=False, stratify=None) # np.savez(&#x27;easybanking_1w.npz&#x27;, x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test) return (x_train, y_train), (x_test, y_test) 模型定义12345678910from tensorflow.keras import layers, modelsdef build_model(n_steps_in, n_features, n_steps_out): model = models.Sequential() model.add(layers.Bidirectional(layers.LSTM(units=64, activation=&#x27;tanh&#x27;), input_shape=(n_steps_in, n_features))) model.add(layers.Dense(n_steps_out)) model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;, metrics=[&#x27;accuracy&#x27;]) return model 模型训练1234567891011121314151617181920212223242526272829303132import matplotlib.pyplot as plt# 构造数据集window_size = 10log_parsed_path = &#x27;easybanking_parsed_1w.csv&#x27;template_path = &#x27;easybanking_template.csv&#x27;(x_train, y_train), (x_test, y_test) = create_vector_dataset(log_parsed_path, template_path, window_size)# 定义模型n_steps_in, n_features, n_steps_out = (x_train.shape[1], x_train.shape[2], y_train.shape[1])model = build_model(n_steps_in, n_features, n_features)model.summary()# 模型训练call_backs = [ callbacks.ModelCheckpoint(&quot;best_model.h5&quot;, save_best_only=True, monitor=&quot;val_loss&quot;), callbacks.ReduceLROnPlateau(monitor=&quot;val_loss&quot;, factor=0.5, patience=20, min_lr=0.0001), callbacks.EarlyStopping(monitor=&quot;val_loss&quot;, patience=50, verbose=1),]history = model.fit(x_train, y_train, batch_size=32, epochs=20, callbacks=call_backs, validation_split=0.2, shuffle=True, verbose=1)# 画图metric = &quot;accuracy&quot;plt.figure()plt.plot(history.history[metric])plt.plot(history.history[&quot;val_&quot; + metric])plt.title(&quot;model &quot; + metric)plt.ylabel(metric, fontsize=&quot;large&quot;)plt.xlabel(&quot;epoch&quot;, fontsize=&quot;large&quot;)plt.legend([&quot;train&quot;, &quot;val&quot;], loc=&quot;best&quot;)plt.show()plt.close()","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"LSTM","slug":"LSTM","permalink":"https://fuyunliu.github.io/tags/LSTM/"},{"name":"Drain3","slug":"Drain3","permalink":"https://fuyunliu.github.io/tags/Drain3/"},{"name":"FastText","slug":"FastText","permalink":"https://fuyunliu.github.io/tags/FastText/"},{"name":"HanLP","slug":"HanLP","permalink":"https://fuyunliu.github.io/tags/HanLP/"}]},{"title":"Session 、Cookie 和 JWT 详解","slug":"session-cookie-jwt","date":"2021-04-17T06:26:02.000Z","updated":"2021-04-17T07:09:29.917Z","comments":true,"path":"2021/04/17/session-cookie-jwt/","link":"","permalink":"https://fuyunliu.github.io/2021/04/17/session-cookie-jwt/","excerpt":"什么是 CookieHTTP Cookie（也叫 Web Cookie 或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的HTTP协议记录稳定的状态信息成为了可能。 Cookie 主要用于以下三个方面： 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等）","text":"什么是 CookieHTTP Cookie（也叫 Web Cookie 或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的HTTP协议记录稳定的状态信息成为了可能。 Cookie 主要用于以下三个方面： 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） 什么是 SessionSession 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。 Session 和 Cookie 的区别 作用范围不同，Cookie 保存在客户端（浏览器），Session 保存在服务器端。 存取方式的不同，Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserId 等。 有效期不同，Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。 存储大小不同， 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie。 Cookie 是客户端技术，Session 是服务端技术，Session 借助 Cookie 存储 sessionid。 如果浏览器禁用 Cookie，可以使用 POST 提交 sessionid 或者使用 JWT 技术。 分布式 Session在互联网公司为了可以支撑更大的流量，后端往往需要多台服务器共同来支撑前端用户请求，那如果用户在 A 服务器登录了，第二次请求跑到服务 B 就会出现登录失效问题。 分布式 Session 一般会有以下几种解决方案： Nginx ip_hash 策略，服务端使用 Nginx 代理，每个请求按访问 IP 的 hash 分配，这样来自同一 IP 固定访问一个后台服务器，避免了在服务器 A 创建 Session，第二次分发到服务器 B 的现象。 Session 复制，任何一个服务器上的 Session 发生改变（增删改），该节点会把这个 Session 的所有内容序列化，然后广播给所有其它节点。 共享 Session，服务端无状态话，将用户的 Session 等信息使用缓存中间件来统一管理，保障分发到每一个服务器的响应结果都一致。 跨域请求同源策略/SOP（Same origin policy）是一种约定，由 Netscape 公司 1995年引入浏览器，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，浏览器很容易受到 XSS、CSFR 等攻击。所谓同源是指”协议+域名+端口”三者相同，即便两个不同的域名指向同一个 ip 地址，也非同源。 解决方案： Nginx 代理 Jsonp 跨域 JWT 认证 JWT 数据结构Encoded1eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c DecodedHeader (头部)1234567&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;, &quot;kid&quot;: &quot;Key ID&quot;, &quot;cty&quot;: &quot;Content Type&quot;, &quot;enc&quot;: &quot;Encrypt Algorithm&quot;&#125; Payload (负载)123456789&#123; &quot;iss&quot;: &quot;issuer(签发人)&quot;, &quot;exp&quot;: &quot;expiration time(过期时间)&quot;, &quot;sub&quot;: &quot;subject(主题)&quot;, &quot;aud&quot;: &quot;audience(受众)&quot;, &quot;nbf&quot;: &quot;Not Before(生效时间)&quot;, &quot;iat&quot;: &quot;Issued At(签发时间)&quot;, &quot;jti&quot;: &quot;JWT ID(编号)&quot;&#125; Signature (签名)1234567HMACSHA256( base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload),your-256-bit-secret) secret base64 encoded JWT = Header.Payload.Signature","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"JWT","slug":"JWT","permalink":"https://fuyunliu.github.io/tags/JWT/"},{"name":"Session","slug":"Session","permalink":"https://fuyunliu.github.io/tags/Session/"},{"name":"Cookie","slug":"Cookie","permalink":"https://fuyunliu.github.io/tags/Cookie/"}]},{"title":"Postgresql Tutorial","slug":"postgresql-tutorial","date":"2021-03-14T09:27:56.000Z","updated":"2021-03-14T09:29:50.578Z","comments":true,"path":"2021/03/14/postgresql-tutorial/","link":"","permalink":"https://fuyunliu.github.io/2021/03/14/postgresql-tutorial/","excerpt":"","text":"psql 数据库名 –连接数据库select rolname,rolpassword from pg_authid;–查看用户名密码select usename,passwd from pg_shadow;–查看用户名密码select version(); – 查看版本select current_database();–查看当前数据库\\l –查看所有数据库\\dt –查看表\\password username –修改密码\\password –设置密码。? –查看psql命令列表。\\c [database_name] –连接其他数据库，切换数据库。\\conninfo –列出当前数据库和连接的信息。\\d –列出当前数据库的所有表格。\\d [table_name] –列出某一张表格的结构。\\du –列出所有用户。\\e –打开文本编辑器。help –帮助\\h –查看SQL命令的解释，比如\\h select。\\q –退出","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://fuyunliu.github.io/tags/PostgreSQL/"}]},{"title":"LSTM 时间序列预测","slug":"lstm-models-for-time-series-forecasting","date":"2020-12-03T07:48:54.000Z","updated":"2020-12-17T01:18:30.188Z","comments":true,"path":"2020/12/03/lstm-models-for-time-series-forecasting/","link":"","permalink":"https://fuyunliu.github.io/2020/12/03/lstm-models-for-time-series-forecasting/","excerpt":"调节内容1.输出维度 units2.时间步长 timesteps3.激活函数 activation4.增加特征：增长率5.增加特征：日期属性6.预测增长率7.单层 stateful LSTM8.双层 stateful LSTM9.批数据大小 batch_size10.训练循环次数 epochs11.原序列对数12.原序列差分13.归一化值域 feature_range14.Bidirectional LSTM15.CNN LSTM16.ConvLSTM","text":"调节内容1.输出维度 units2.时间步长 timesteps3.激活函数 activation4.增加特征：增长率5.增加特征：日期属性6.预测增长率7.单层 stateful LSTM8.双层 stateful LSTM9.批数据大小 batch_size10.训练循环次数 epochs11.原序列对数12.原序列差分13.归一化值域 feature_range14.Bidirectional LSTM15.CNN LSTM16.ConvLSTM 原始数据集 原始序列超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义12345678910111213# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps,features)))model.add(Dense(1))model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]# 训练history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_valid, y_valid), callbacks=callbacks, verbose=1) 验证损失12345validate cost: 0.00031516602518997155elapsed time: 2.2270960807800293 (s)Train Score: 2350.04 RMSEValid Score: 1975.81 RMSETest Score: 1143.96 RMSE Stateful LSTM使 RNN 具有状态意味着每批样品的状态将被重新用作下一批样品的初始状态。 stateful LSTM：能让模型学习到你输入的samples之间的时序特征，适合一些长序列的预测，哪个sample在前，那个sample在后对模型是有影响的。stateless LSTM：输入samples后，默认就会shuffle，可以说是每个sample独立，之间无前后关系，适合输入一些没有关系的样本。 超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义12345678910111213141516# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, stateful=True, batch_input_shape=(batch_size, timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练for i in range(epochs): history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) model.reset_states() 验证损失123456789validate cost: 0.0003053233659405426elapsed time: 8.588087797164917 (s)Train Score: 2098.81 RMSEValid Score: 1768.38 RMSETest Score: 880.83 RMSE# stateful带来了一些改观，从理论上讲stateful会有些好处，因为我们的数据是具有自相关性的时序数据。# 但是stateful限制训练、验证和预测必须接受以batch为单位的数据，给后面预测带来很大变动和限制，故不使用stateful。 序列对数超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义123456789101112131415# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 验证损失1234567validate cost: 0.0003706229881521907elapsed time: 2.328054904937744 (s)Train Score: 2451.66 RMSEValid Score: 1843.60 RMSETest Score: 901.27 RMSE# 取对数在验证集和测试集上有所改观，在训练集上误差稍微增大 序列差分超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义1234567891011121314# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 一阶差分1234567891011validate cost: 9.300382513113658e-05elapsed time: 1.9793977737426758 (s)Train Score: 1868.55 RMSEValid Score: 1608.05 RMSETest Score: 884.86 RMSE# 一阶差分误差改善效果明显，相较于取对数更优。# 经过验证发现将归一化值域调整为 feature_range = (-1, 1) 之后# 验证集损失 val_loss 会增大，但是 Train, Valid, Test 的 RMSE 却减少 100-200左右# 经过差分之后的数据有正有负，似乎 (-1, 1) 是合理的，但是本文将保持 (0, 1) 不变# 研究其他变化带来的变化，最后选出最优的特征和超参数，归一化值域可随时更改 二阶差分1234567validate cost: 0.00017968161298761821elapsed time: 2.5699269771575928 (s)Train Score: 2607.74 RMSEValid Score: 2270.10 RMSETest Score: 1025.25 RMSE# 二阶差分反而更差了，故只取一阶差分 序列对数差分超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义123456789101112131415# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 验证损失12345678910validate cost: 8.005627370742416e-05elapsed time: 2.5233328342437744 (s)Train Score: 1883.97 RMSEValid Score: 1621.68 RMSETest Score: 879.93 RMSE# 可以发现对数差分的效果几乎和一阶差分的效果是一样的，取对数的效果微乎其微。# 差分之后的序列更接近平稳序列，可能更有利于LSTM建模预测。# 取对数只是缩小值域，再说还有归一化操作，似乎不是必要的。# 比如像LightGBM算法并没有归一化操作，所以取对数有比较好的调参效果。 增加日期属性 超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 4 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义123456789101112131415# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 验证损失1234567validate cost: 0.002384760812697476elapsed time: 1.893902063369751 (s)Train Score: 2104.13 RMSEValid Score: 5434.98 RMSETest Score: 5839.40 RMSE# 模型在训练集、验证集和测试集表现越来越差 增加增长率 超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 2 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义123456789101112131415# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 验证损失1234567891011validate cost: 0.00023447876570476951elapsed time: 2.060291051864624 (s)Train Score: 2084.47 RMSEValid Score: 1704.23 RMSETest Score: 1039.05 RMSE# 增长率也可以改善预测结果，原理和一阶差分相似，差分是做减法，增长率是做除法。# 误差减小了，但是增长率产生了负数的情况。# 之前的实验是增长率结合stateful LSTM，得到的结果是：增长率可以和一阶差分类似改善拟合结果，并无发现有负数的情况。# 做完实验考虑预测部分的时候发现stateful必须接受batch为单位的数据，使得预测变得麻烦，改动工作较大。 预测增长率变换目标值为增长率，预测增长率，和前一天的交易量进行计算，得到当前的交易预测值。 123456789validate cost: 0.007923171162502511elapsed time: 1.5544190406799316 (s)Train Score: 1575.60 RMSEValid Score: 1307.57 RMSETest Score: 761.17 RMSE# 结合图可以看出，相比增加增长率这个特征，直接预测增长率收到了比较好的效果。# 而且没有出现负数的情况，拟合误差减少很多，可以采用。 双层LSTM超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 单层LSTM有Dropout123456789101112131415# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features)))model.add(Dropout(0.2))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 123456validate cost: 0.00024637159007593934elapsed time: 2.1154732704162598 (s)Train Score: 2107.65 RMSEValid Score: 1746.91 RMSETest Score: 906.77 RMSE 双层LSTM有Dropout123456789101112131415161718# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features), return_sequences=True))model.add(Dropout(0.2))model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features)))model.add(Dropout(0.2))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 123456validate cost: 0.00030157841621581707elapsed time: 3.7397210597991943 (s)Train Score: 2266.96 RMSEValid Score: 1932.75 RMSETest Score: 1101.70 RMSE 双层LSTM无Dropout12345678910111213141516# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features), return_sequences=True))model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 123456789validate cost: 0.00028111153606354833elapsed time: 3.16672682762146 (s)Train Score: 2269.73 RMSEValid Score: 1866.01 RMSETest Score: 1003.38 RMSE# 单层LSTM加上Dropout正则化层，相较于单层LSTM似乎有所改善，但对于Dropout的作用还不太明白。# 双层LSTM加上Dropout正则化层，相较于双层LSTM又是略差些，但是相差不大。# 对于这些结果，可能需要多次实验取均值来确定哪些因素对拟合更优。 差分结合增长率在一阶差分的基础上增加增长率这个特征 超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 2 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义1234567891011121314# 模型model = Sequential()model.add(LSTM(units=units, activation=&#x27;tanh&#x27;, input_shape=(timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 验证损失12345678910validate cost: 8.446127437506554e-05elapsed time: 2.3685760498046875 (s)Train Score: 1787.02 RMSEValid Score: 1532.42 RMSETest Score: 901.20 RMSE# 从误差结果可以看出，这个和一阶差分的结果相差无几# 但是从拟合图上看，拟合值出现了负数的情况，这是增长率带来的结果# 所以可以通过预测增长率，再结合一阶差分看看 预测增长率预测增长率，再结合一阶差分，这个和上面的不同之处在于使用增长率作为目标值。 12345678validate cost: 0.007926768652017368elapsed time: 1.7802140712738037 (s)Train Score: 1600.33 RMSEValid Score: 1325.91 RMSETest Score: 769.42 RMSE# 这个结果和单纯预测增长率的结果又是相差无几，差不差分都无所谓了。# 总结：一阶差分和预测增长率是二选一的结果，二者都能达到较好的优化，预测增长率更优一些。 Bidirectional LSTM On some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations.This is called a Bidirectional LSTM.We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional. 超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义123456789101112131415# 模型model = Sequential()model.add(Bidirectional(LSTM(units=units, activation=&#x27;tanh&#x27;), input_shape=(timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 验证损失1234567validate cost: 0.0002106719806980023elapsed time: 2.6818039417266846 (s)Train Score: 1946.72 RMSEValid Score: 1615.40 RMSETest Score: 823.85 RMSE# Bidirectional LSTM 具有明显改善效果，这个可以结合一阶差分或者预测增长率使用。 CNN LSTM A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data.The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data.A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM. 超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 4 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义123456789101112131415161718# 模型model = Sequential()model.add(TimeDistributed(Conv1D(filters=units, kernel_size=1, activation=&#x27;tanh&#x27;), input_shape=(None, timesteps//2, features)))model.add(TimeDistributed(MaxPooling1D(pool_size=2)))model.add(TimeDistributed(Flatten()))model.add(LSTM(units=units, activation=&#x27;tanh&#x27;))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 验证损失123456validate cost: 0.00021777707989074557elapsed time: 2.1609442234039307 (s)Train Score: 2035.46 RMSEValid Score: 1642.41 RMSETest Score: 851.72 RMSE ConvLSTM A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading of input is built directly into each LSTM unit.The ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting. 超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 4 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义12345678910111213141516# 模型model = Sequential()model.add(ConvLSTM2D(filters=units, kernel_size=(1,2), activation=&#x27;tanh&#x27;, input_shape=(timesteps//2, 1, timesteps//2, features)))model.add(Flatten())model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 验证损失12345678validate cost: 0.00022352560738618104elapsed time: 3.2140681743621826 (s)Train Score: 2092.71 RMSEValid Score: 1663.95 RMSETest Score: 844.92 RMSE# 三种模型相较于原始的单层 stateless LSTM 模型都具有改善效果# Bidirectional LSTM 略好一点 优化方案Stateless LSTM or Bidirectional LSTM or CNN LSTM or ConvLSTM 一阶差分 or 预测增长率 本次实验使用颜色标记的方案 超参数12345678# 定义超参数batch_size = 128 # 批数据大小epochs = 50 # 实验训练次数units = 64 # 状态的输出维度features = 1 # 最终输出维度timesteps = 3 # 时间步长feature_range = (0, 1) # 归一化值域ratio = 0.8 # 数据集分割比例 模型定义123456789101112131415# 模型model = Sequential()model.add(Bidirectional(LSTM(units=units, activation=&#x27;tanh&#x27;), input_shape=(timesteps, features)))model.add(Dense(1))model.summary()callbacks = [ keras.callbacks.EarlyStopping(monitor=&#x27;val_loss&#x27;, patience=2), keras.callbacks.ModelCheckpoint(filepath=&#x27;model.h5&#x27;, monitor=&#x27;val_loss&#x27;, save_best_only=True), keras.callbacks.ReduceLROnPlateau(monitor=&#x27;val_loss&#x27;, factor=0.01, patience=2)]model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;)# 训练history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), shuffle=False, callbacks=callbacks) 验证损失123456789validate cost: 0.00794555525360681elapsed time: 2.404299020767212 (s)Train Score: 1576.97 RMSEValid Score: 1307.11 RMSETest Score: 758.50 RMSE# 可以看出这个方案和预测增长率的方案的结果是一样的，改变为Bidirectional LSTM 结构有略微的好处# 这种双向循环神经网络的隐藏层保存了两个值，A 参与正向计算， A&#x27; 参与反向计算，最终的输出值 y 取决于 A 和 A&#x27; 看看细节 成功率123456validate cost: 6.0617386995090114e-09elapsed time: 3.4173359870910645 (s)Train Score: 0.05 RMSEValid Score: 0.01 RMSETest Score: 0.13 RMSE 响应时间123456validate cost: 0.009244604190229511elapsed time: 3.7244749069213867 (s)Train Score: 56.79 RMSEValid Score: 14.35 RMSETest Score: 77.87 RMSE 新的问题Why is my forecasted time series one step behind the actual time series?Github上有人给出的一种解释是：这是由于序列存在自相关性 做过时间序列的朋友可能常常会有这样的感受，用了某种算法做出来的测试集的平均绝对误差率或者r2系数都很好，但是把测试集的真实值及预测值画出来对比一下，就会发现t时刻的预测值往往是t-1时刻的真实值，也就是模型倾向于把上一时刻的真实值作为下一时刻的预测值，导致两条曲线存在滞后性，也就是真实值曲线滞后于预测值曲线，就像下图右边所显示的那样。之所以会这样，是因为序列存在自相关性，如一阶自相关指的是当前时刻的值与其自身前一时刻值之间的相关性。因此，如果一个序列存在一阶自相关，模型学到的就是一阶相关性。而消除自相关性的办法就是进行差分运算，也就是我们可以将当前时刻与前一时刻的差值作为我们的回归目标 简单的说就是特征值X包含了目标值Y，试试改为一阶差分结果作为Y，上面已经试过增长率作为Y了，结果就是误差还好，但是有负数的情况出现。 如何解决存在预测滞后的现象是因为时间序列本身存在自相关性，因为损失函数是mse，模型倾向于把上一个时刻的值当作下一个时刻的预测值，导致图形画出来看似很好，mse也很小。解决办法是消除时间序列的自相关性，可以进行差分或者分解，分解方法有EMD分解和小波分解法，上面试过差分似乎还是存在预测值滞后的问题，试过使用EMD分解一周的响应时间，性能不好，效率不高，耗时很久，效果还挺好，分解出了27个IMF分量。另外我需要证明LSTM在预测非平稳时间序列，也就是不存在自相关性的时间序列上，不存在预测值滞后的问题，我想最简单的就是构造一个一正一负的时间序列，如果LSTM模型总是拿上一个时刻的值当作下一个时刻的预测值，那么这个模型预测可以说总是错的，如果结果还好，不存在滞后问题，那么就可以使用EMD分解法来逐个预测，最后综合各个的预测结果，还有一个问题是EMD分解性能的问题，可能需要再找一个效率更高的包，可能存在也可能不存在。如果顺利的话，最后预测也是个问题，既然分解了，那么预测的时候怎么预测？ 基于EMD分解与LSTM的空气质量预测 我构造了一正一负的时间序列 正：100～150 负：-150～-100 拟合结果 再拉清楚点看看 完全重合，没有错位，没有滞后。是不是说明在平稳的非自相关性的时间序列上就不会有预测值滞后的问题，而并非特征构造的问题，因为LSTM本身就是利用过去的值来预测未来的值，要不然有个timesteps参数作何说明。","categories":[],"tags":[{"name":"LSTM","slug":"LSTM","permalink":"https://fuyunliu.github.io/tags/LSTM/"}]},{"title":"Instagram 的分片与 ID 设计","slug":"sharding-ids-at-instagram","date":"2020-06-03T02:50:07.000Z","updated":"2020-06-12T08:41:34.656Z","comments":true,"path":"2020/06/03/sharding-ids-at-instagram/","link":"","permalink":"https://fuyunliu.github.io/2020/06/03/sharding-ids-at-instagram/","excerpt":"Instagram上有大量的数据，每分钟就有超过25张的图片和90个点赞。为了确保所有重要的数据都能被合理存储并且及时得被提取应用，我们对数据进行了分片（sharding）——也就是说，我们把数据放到多个桶（bucket）中，每个桶里都有一部分数据。 我们的应用服务器上运行的是Django， 后端数据库是PostgreSQL。对数据分片首先要决定是否要保留PostgreSQL作为主要的数据存储库，是否要采用其他的数据库。经过评估一些不同的数据库解决方案，我们最终确定最适合的方案是在PostgreSQL数据库集群上实现数据分片。 然而在把数据写到数据库之前，我们还要解决如何给数据（例如Instagram上发布的没一张图片）加上唯一标识符的问题。在单一数据库上的典型解法——使用数据库自带的自增主键功能——在当数据需要被同时插入到多个数据库时就不适用了。文章的下面就来讲讲我们是如何解决这个问题的","text":"Instagram上有大量的数据，每分钟就有超过25张的图片和90个点赞。为了确保所有重要的数据都能被合理存储并且及时得被提取应用，我们对数据进行了分片（sharding）——也就是说，我们把数据放到多个桶（bucket）中，每个桶里都有一部分数据。 我们的应用服务器上运行的是Django， 后端数据库是PostgreSQL。对数据分片首先要决定是否要保留PostgreSQL作为主要的数据存储库，是否要采用其他的数据库。经过评估一些不同的数据库解决方案，我们最终确定最适合的方案是在PostgreSQL数据库集群上实现数据分片。 然而在把数据写到数据库之前，我们还要解决如何给数据（例如Instagram上发布的没一张图片）加上唯一标识符的问题。在单一数据库上的典型解法——使用数据库自带的自增主键功能——在当数据需要被同时插入到多个数据库时就不适用了。文章的下面就来讲讲我们是如何解决这个问题的 开始前，我们先列出系统所需要的所有重要的功能。 产生的数据ID需是可以按时间排序的。（比如对一列图片数据的ID进行排序，可以不需要提取太多图片本身的信息） 理想的ID是64位的。（这样索引更小，存储也更优，像Redis） 系统要尽量少的引用“可变动因素”——在很少工程师的情况下还可以扩张Instagram的很大一部分原因就是，我们相信简单易懂的方案。 现有的解决方案由Web应用层生成ID这种方案将ID的生成完全交到应用层，而不是数据库。例如，MongoDB的ObjectId，就是12字节长并且在最前面加上时间戳进行编码。另一个流行的方案是使用UUIDs。 优点： 每个应用线程独立生成ID，最小的降低ID生成的失败和竞争。 如果用时间戳作为ID的起始部分，那么ID可以按时间排序。 缺点： 通常需要更多的存储空间（96位或者更多）来确保ID的合理唯一性。 一些UUID类型完全是随机的，无法排序。 通过单独的服务产生ID例如：Twitter的Snowflake，是一个Thrift服务，使用了Apache Zookeeper来协调各个结点并且产生64为的唯一ID。 优点： Snowflake的ID只有64位，是UUID的一半。 可以放时间戳到ID头，从而可以按时间排序。 分布式系统保证了系统结点不会挂掉。 缺点： 增加了复杂性，而且引入了更多的“可变动因素”（如ZooKeeper, Snowflake服务器）到系统构架中。 数据库票据（DB Ticket）服务器利用数据库自带的自增特性来确保唯一性。Flicker采用这一方法，不过还用了两台ticket数据库（一个生成偶数，一个生成奇数）来避免单点失败。 优点： 数据库好理解，带有易预测的可扩张功能。 缺点： 最终会出现数据写入的瓶颈（尽管Flicker称在高扩展下没有问题）。 需要管理多的两台服务器（或者EC2实例）。 如果单独使用数据库，会出现单点失效。如果使用多个数据库，则不能保证ID可以按时间排序。 在所有这些方案中，Twitter的snowflake是最接近的，但是生成ID所需的添加复杂性又和我们的目标冲突。我们的替换方案是采用概念上相近的方法，但是带到PostgreSQL内部实现。 Instagram 的方案我们的分片系统是由上千个“逻辑”分片组成的，由代码映射到少量的物理分片。 通过这个方法，我们一开始用少数数据库实现，慢慢扩展到更多个数据库，只需要把部分逻辑分片从一台数据库转移到另一台数据库里，不需要重新把数据重新聚合。我们用到的PostgreSQL的schema的特性可以轻松的实现计划和管理。 Schema（不要和SQL单个表的schema搞混了）是PostgreSQL里的一个逻辑分组功能。每一个PostgreSQL数据库都有好几个schema，每一个schema都有一到多个表。表名在没个schema中都是唯一的，而不是每个数据库，默认情况下，PostgreSQL会把所有数据都放在一个叫“public”的schema中。 在我们的系统中每个“逻辑”分片都是一个PostgreSQL schema， 每个分片的表（比如照片的“点赞”功能）都存在每个schema中。 我们通过使用PL/PGSQL, PostgreSQL内部的编程语言，和PostgreSQL现有的自增功能来生成ID。 每个ID都由下面几个部分组成： 12341位的毫秒级时间（用于产生41年的ID）13位用来表示逻辑分片的ID10位的自增序列，模上1024， 意味着每个分片每毫秒可以产生1024个ID 下面通过一个例子说明：比如说现在是2011年的九月九日，我们的纪元是从2011年的一月一号开始。从新纪元开始到此有1387263000毫秒，那么我们把这个数字左移41位来填满ID的头。 id = 1387263000 &lt;&lt;(64 – 41) 接下来， 我们拿来我们准备把这个数据插入的分片ID；如果我们用户ID是31341， 这个分片的ID是 31341%2000 → 1341。 我们接下来把下面13为填满： id |= 1341 &lt;&lt; (64-41-13) 最后， 我们用所剩的自增序列（每个schema中每个表中这个序列都是唯一的）来填满的后面的位数。 假设这张表中已经有了5000个ID； 我们下一个数据就是5001，我们把它模上1024得到： id |= （5001%1024） 我们就得到了我们的ID， 我们把这个id作为insert中的RETURNING返回给应用层。 下面是是实现以上过程的PL/PGSQL代码（这里用的schema是insta5） 1234567891011121314CREATE OR REPLACE FUNCTION insta5.next_id(OUT result bigint) AS $$DECLARE our_epoch bigint := 1314220021721; seq_id bigint; now_millis bigint; shard_id int := 5;BEGIN SELECT nextval(&#x27;insta5.table_id_seq&#x27;) %% 1024 INTO seq_id; SELECT FLOOR(EXTRACT(EPOCH FROM clock_timestamp()) * 1000) INTO now_millis; result := (now_millis - our_epoch) &lt;&lt; 23; result := result | (shard_id &lt;&lt; 10); result := result | (seq_id);END;$$ LANGUAGE PLPGSQL; 要生成表示执行下面的部分： 1234CREATE TABLE insta5.our_table( &quot;id&quot; bigint NOT NULL DEFAULT insta5.next_id(), ... rest of table schema ...) 成了！我们得到了应用中唯一的主键（还有一个好处是，ID中包含了分片ID可以用来轻松映射）。我们已经把这个方法用在产品中，并且对结果感到非常满意。 本文来自：Instagram 的分片与 ID 设计","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://fuyunliu.github.io/tags/Algorithm/"}]},{"title":"Tmux 使用教程","slug":"tmux-tutorial","date":"2020-05-29T07:03:16.000Z","updated":"2020-05-29T09:05:06.646Z","comments":true,"path":"2020/05/29/tmux-tutorial/","link":"","permalink":"https://fuyunliu.github.io/2020/05/29/tmux-tutorial/","excerpt":"Tmux 是一个终端复用工具，和 screen 一样，screen 相对简单好使，tmux 更高级。 安装12345# CentOSyum install -y tmux# MacOSbrew install tmux","text":"Tmux 是一个终端复用工具，和 screen 一样，screen 相对简单好使，tmux 更高级。 安装12345# CentOSyum install -y tmux# MacOSbrew install tmux 基本操作1234567891011121314151617181920212223242526272829303132# 新建无名称会话tmux# 新建会话tmux new -s demo# 挂起会话tmux detach# 默认进入第一个会话tmux a# 进入名为demo的会话tmux a -t demo# 关闭demo会话tmux kill-session -t demo# 关闭服务器tmux kill-server# 查看会话tmux list-sessiontmux ls# 切换会话tmux switch -t 0 # 使用会话编号tmux switch -t demo # 使用会话名称# 重命名会话tmux rename-session -t demo new-demo 系统指令 前缀 指令 描述 prefix ? 显示快捷键帮助文档 prefix d 断开当前会话 prefix D 选择要断开的会话 prefix Ctrl+z 挂起当前会话 prefix r 强制重载当前会话 prefix s 显示会话列表用于选择并切换 prefix : 进入命令行模式 prefix [ 进入复制模式，按q退出 prefix ] 粘贴复制模式中复制的文本 prefix ~ 列出提示信息缓存 窗口指令 前缀 指令 描述 prefix c 新建窗口 prefix &amp; 关闭当前窗口（关闭前需输入y or n确认） prefix 0-9 切换到指定窗口 prefix p 切换到上一窗口 prefix n 切换到下一窗口 prefix w 打开窗口列表，用于且切换窗口 prefix , 重命名当前窗口 prefix . 修改当前窗口编号（适用于窗口重新排序） prefix f 快速定位到窗口（输入关键字匹配窗口名称） 面板指令 前缀 指令 描述 prefix “ 当前面板上下一分为二，下侧新建面板 prefix % 当前面板左右一分为二，右侧新建面板 prefix x 关闭当前面板（关闭前需输入y or n确认） prefix z 最大化当前面板，再重复一次按键后恢复正常（v1.8版本新增） prefix ! 将当前面板移动到新的窗口打开（原窗口中存在两个及以上面板有效） prefix ; 切换到最后一次使用的面板 prefix q 显示面板编号，在编号消失前输入对应的数字可切换到相应的面板 prefix { 向前置换当前面板 prefix } 向后置换当前面板 prefix Ctrl+o 顺时针旋转当前窗口中的所有面板 prefix 方向键 移动光标切换面板 prefix o 选择下一面板 prefix 空格键 在自带的面板布局中循环切换 prefix Alt+方向键 以5个单元格为单位调整当前面板边缘 prefix Ctrl+方向键 以1个单元格为单位调整当前面板边缘（Mac下被系统快捷键覆盖） prefix t 显示时钟 自定义配置编辑 ~/.tmux.conf 添加如下内容 12345678910111213141516171819202122232425262728# prefix configurationset -g prefix C-aunbind C-bbind C-a send-prefix# split windowunbind &#x27;&quot;&#x27;bind - split-window -v -c &#x27;#&#123;pane_current_path&#125;&#x27;unbind %bind = split-window -h -c &#x27;#&#123;pane_current_path&#125;&#x27;# mouse onset-option -g mouse on# pane navigationbind -r k select-pane -Ubind -r j select-pane -Dbind -r h select-pane -Lbind -r l select-pane -R# pane resizingbind -r ^k resize-pane -U 2bind -r ^j resize-pane -D 2bind -r ^h resize-pane -L 2bind -r ^l resize-pane -R 2# reload configurationbind r source-file ~/.tmux.conf \\; display &#x27;~/.tmux.conf sourced&#x27;","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"CentOS","slug":"CentOS","permalink":"https://fuyunliu.github.io/tags/CentOS/"}]},{"title":"Sqlalchemy 基本用法","slug":"sqlalchemy-usage","date":"2019-05-12T07:18:31.000Z","updated":"2020-05-29T09:11:11.524Z","comments":true,"path":"2019/05/12/sqlalchemy-usage/","link":"","permalink":"https://fuyunliu.github.io/2019/05/12/sqlalchemy-usage/","excerpt":"Sqlalchemy 基本用法","text":"Sqlalchemy 基本用法 通用导入12345678910from sqlalchemy import create_enginefrom sqlalchemy.orm import scoped_session, sessionmakerfrom sqlalchemy import Column, Integer, String, ForeignKey, Booleanfrom sqlalchemy.orm import relationshipfrom sqlalchemy.ext.declarative import declarative_baseengine = create_engine(&#x27;sqlite:///test.db&#x27;, echo=True)Base = declarative_base()db_session = scoped_session(sessionmaker(bind=engine))Base.query = db_session.query_property() 一对一1234567891011121314class Parent(Base): __tablename__ = &#x27;parent&#x27; id = Column(Integer, primary_key=True) name = Column(String) child_id = Column(Integer, ForeignKey(&#x27;child.id&#x27;))class Child(Base): __tablename__ = &#x27;child&#x27; id = Column(Integer, primary_key=True) name = Column(String) parent = relationship(&#x27;Parent&#x27;, backref=&#x27;child&#x27;, uselist=False) 一对多12345678910111213141516171819# the one sideclass Parent(Base): __tablename__ = &#x27;parent&#x27; id = Column(Integer, primary_key=True) name = Column(String) # children = relationship(&quot;Child&quot;, back_populates=&quot;parent&quot;) children = relationship(&quot;Child&quot;, backref=&quot;parent&quot;, lazy=&quot;dynamic&quot;)# the many sideclass Child(Base): __tablename__ = &#x27;child&#x27; id = Column(Integer, primary_key=True) name = Column(String) parent_id = Column(Integer, ForeignKey(&#x27;parent.id&#x27;)) # parent = relationship(&quot;Parent&quot;, back_populates=&quot;children&quot;) # parent = relationship(&quot;Parent&quot;, backref=&quot;children&quot;) 多对一12345678910111213141516# the many sideclass Parent(Base): __tablename__ = &#x27;parent&#x27; id = Column(Integer, primary_key=True) name = Column(String) child_id = Column(Integer, ForeignKey(&#x27;child.id&#x27;))# the one sideclass Child(Base): __tablename__ = &#x27;child&#x27; id = Column(Integer, primary_key=True) name = Column(String) parents = relationship(&#x27;Parent&#x27;, backref=&#x27;child&#x27;, lazy=&#x27;dynamic&#x27;) 多对多123456789101112131415161718192021222324252627282930class Department(Base): __tablename__ = &#x27;department&#x27; id = Column(Integer, primary_key=True) name = Column(String) employees = relationship( &#x27;Employee&#x27;, secondary=&#x27;department_employee_link&#x27; )class Employee(Base): __tablename__ = &#x27;employee&#x27; id = Column(Integer, primary_key=True) name = Column(String) hired_on = Column( DateTime, default=func.now()) departments = relationship( &#x27;Department&#x27;, secondary=&#x27;department_employee_link&#x27; )class DepartmentEmployeeLink(Base): __tablename__ = &#x27;department_employee_link&#x27; department_id = Column(Integer, ForeignKey(&#x27;department.id&#x27;), primary_key=True) department = relationship(&#x27;Department&#x27;) employee_id = Column(Integer, ForeignKey(&#x27;employee.id&#x27;), primary_key=True) employee = relationship(&#x27;Employee&#x27;) 自身多对多1234567891011121314151617181920212223class Follow(Base): __tablename__ = &#x27;me_follow_you&#x27; me_id = Column(Integer, ForeignKey(&#x27;users.id&#x27;), primary_key=True) me = relationship(&#x27;User&#x27;, foreign_keys=[me_id]) you_id = Column(Integer, ForeignKey(&#x27;users.id&#x27;), primary_key=True) you = relationship(&#x27;User&#x27;, foreign_keys=[you_id]) created = Column(DateTime(timezone=True), default=datetime.utcnow)class User(Base): __tablename__ = &#x27;users&#x27; id = Column(Integer, primary_key=True) name = Column(String(64)) # stars=我关注的人 fans=我的粉丝 stars = relationship(&#x27;User&#x27;, secondary=&#x27;me_follow_you&#x27;, primaryjoin=&#x27;User.id==Follow.me_id&#x27;, secondaryjoin=&#x27;User.id==Follow.you_id&#x27;, backref=backref(&#x27;fans&#x27;, lazy=&#x27;dynamic&#x27;), lazy=&#x27;dynamic&#x27;) 自身一对一123456class Node(Base): __tablename__ = &#x27;node&#x27; id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey(&#x27;node.id&#x27;)) data = Column(String(50)) parent = relationship(&quot;Node&quot;, remote_side=[id]) 创建表1234def init_db(): from sqlalchemy import create_engine engine = create_engine(&#x27;sqlite:///test.db&#x27;, echo=True) Base.metadata.create_all(engine) backref 和 back_populates1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253Parent 下添加 `children = relationship(&quot;Child&quot;, back_populates=&quot;parent&quot;)`创建p1 = Parent()和c1 = Child()失败，原因是One or more mappers failed to initialize，即back_populates必须在关系两端同时指定Parent下添加 `children = relationship(&quot;Child&quot;, back_populates=&quot;parent&quot;)`Child下添加 `parent = relationship(&quot;Parent&quot;, back_populates=&quot;children&quot;)`Parent Attribute:Parent.children Parent.id Parent.metadata Parent.name Parent.queryChild Attribute:Child.id Child.metadata Child.name Child.parent Child.parent_id Child.queryp1 = Parent()c1 = Child()c1.parent = p1 or p1.children.append(c1)Parent下添加 `children = relationship(&quot;Child&quot;, backref=&quot;parent&quot;)`Parent Attribute:Parent.children Parent.id Parent.metadata Parent.name Parent.queryChild Attribute:Child.id Child.metadata Child.name Child.parent_id Child.queryp1 = Parent()c1 = Child()c1.parent = p1 or p1.children.append(c1)可以看出使用backref时，实例化c1时会自动在c1对象上添加parent属性此后再检查:hasattr(Child, &#x27;parent&#x27;) // Truehasattr(c1, &#x27;parent&#x27;) // Truehasattr(Parent, &#x27;children&#x27;) // Truehasattr(p1, &#x27;children&#x27;) // TrueChild 下添加 `parent = relationship(&quot;Parent&quot;, backref=&quot;children&quot;)` 情况和 3 相同Parent下添加 `children = relationship(&quot;Child&quot;, backref=&quot;parent&quot;)`Child下添加 `parent = relationship(&quot;Parent&quot;, backref=&quot;children&quot;)`创建p1 = Parent()和c1 = Child()失败，原因是One or more mappers failed to initialize因此两者只能使用其中之一lazy 指定如何加载相关记录，默认值是&quot;select&quot; select 首次访问时按需加载 immediate 源对象加载后就加载 joined 加载记录,但使用联结 subquery 立即加载,但使用子查询 noload 永不加载 dynamic 不加载记录,但提供加载记录的查询lazy = &quot;dynamic&quot;只能用于collections，不立即查询出结果集，而是提供一系列结果集的方法，可以基于结果集再次进行更精确的查找 default 和 server_default default 是在 ORM 层设置默认值，server_default 是在表结构上设置默认值 onupdate 在 ORM 层生效，server_onupdate 在数据库生效，在 MySQL 上 ON UPDATE 是MySQL在背后创建了 trigger，而在 PostgreSQL 上你必须手动创建 trigger 1234567891011121314from datetime import datetimefrom sqlalchemy import func, sql, textclass Record(Base): __tablename__ = &#x27;records id = Column(Integer, primary_key=True) name = Column(String(64), server_default=text(&#x27;name&#x27;)) created = Column(DateTime(timezone=True), default=datetime.utcnow) # created = Column(DateTime(timezone=True), server_default=func.now()) # created = Column(DateTime(timezone=True), server_default=func.current_timestamp()) updated = Column(DateTime(timezone=True), server_default=func.current_timestamp(), onupdate=func.current_timestamp()) deleted = Column(Boolean, default=False) # deleted = Column(Boolean, server_default=sql.expression.false()) 为flask_sqlalchemy扩展BaseQuery方法12345678910from flask_sqlalchemy import SQLAlchemy, BaseQueryfrom sqlalchemy import funcclass CustomQuery(BaseQuery): def count_all(self): return self.with_entities(func.count()).scalar()db = SQLAlchemy(query_class=CustomQuery)","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://fuyunliu.github.io/tags/Sqlalchemy/"}]},{"title":"二叉树","slug":"treenode","date":"2019-01-15T05:52:53.000Z","updated":"2019-08-06T14:05:27.569Z","comments":true,"path":"2019/01/15/treenode/","link":"","permalink":"https://fuyunliu.github.io/2019/01/15/treenode/","excerpt":"LeetCode 二叉树题解汇总","text":"LeetCode 二叉树题解汇总 二叉树定义12345class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None 相同的树1234567def is_same_tree(p, q): if p is None: return not q if q is None: return not p return p.val == q.val and is_same_tree(p.left, q.left) and is_same_tree(p.right, q.right) 对称的树12345678910111213def is_symmetric(root): if not root: return True return symmetric(root.left, root.right)def symmetric(l1, l2): if l1 is None: return not l2 if l2 is None: return not l1 return l1.val == l2.val and symmetric(l1.left, l2.right) and symmetric(l1.right, l2.left) 层次遍历给定一个二叉树，返回其按层次遍历的节点值。 1234567891011121314def add(node, level, res): if node is None: return if len(res) &lt; level: res.append([]) res[level - 1].append(node.val) add(node.left, level + 1, res) add(node.right, level + 1, res)def level_order(root): res = [] add(root, 1, res) return res 最大深度给定一个二叉树，找出其最大深度。 二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 123def max_depth(root): return 1 + max(map(max_depth, (root.left, root.right))) if root else 0 最小深度给定一个二叉树，找出其最小深度。 最小深度是从根节点到最近叶子节点的最短路径上的节点数量。 12345678910111213141516def min_depth(root): if root is None: return 0 if root.left is None: return 1 + min_depth(root.right) if root.right is None: return 1 + min_depth(root.left) return 1 + min(map(min_depth, (root.left, root.right)))# 更简洁的写法def min_depth(root): if root is None: return 0 depth_under_root = map(min_depth, (root.left, root.right)) return 1 + (min(depth_under_root) or max(depth_under_root)) 将有序数组转化为二叉树将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。 高度平衡二叉树是指一个二叉树每个节点的左右两个子树的高度差的绝对值不超过1。 12345678def sorted_array_to_balanced_tree(nums): if not nums: return None mid = len(nums) // 2 root = TreeNode(nums[mid]) root.left = sorted_array_to_balanced_tree(nums[:mid]) root.right = sorted_array_to_balanced_tree(nums[mid + 1:]) 平衡二叉树一个二叉树每个节点的左右两个子树的高度差的绝对值不超过1。 12345678910def hight(node): if node is None: return 0 return 1 + max(map(hight, (node.left, node.right)))def is_balanced(root): if root is None: return True return abs(hight(root.left) - hight(root.right)) &lt;= 1 and is_balanced(root.left) and is_balanced(root.right) 路径总和给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和。 1234567def has_path_sum(root, sums): if root is None: return False if root.left or root.right: return has_path_sum(root.left, sums - root.val) or has_path_sum(root.right, sums - root.val) return sums == root.val","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"}]},{"title":"Python 处理 Csv 文件常见错误","slug":"csv-error","date":"2018-12-03T02:53:31.000Z","updated":"2020-05-29T09:08:21.774Z","comments":true,"path":"2018/12/03/csv-error/","link":"","permalink":"https://fuyunliu.github.io/2018/12/03/csv-error/","excerpt":"在用 Python 处理 csv 文件时遇到2个错误，记录下处理方法。","text":"在用 Python 处理 csv 文件时遇到2个错误，记录下处理方法。 字段包含 NULL 值csv 文件中字段包含 NULL 值会出错，解决方法是读取文件时把 NULL 值替换为空字符串。 12345import csvwith open(&#x27;test.csv&#x27;, &#x27;rt&#x27;, encoding=&#x27;utf-8&#x27;) as f: fc = csv.DictReader((line.replace(&#x27;\\0&#x27;, &#x27;&#x27;) for line in f)) # do something with fc OverflowError and maxInt123456789101112131415161718192021import csvimport sysmaxInt = sys.maxsizedecrement = Truewhile decrement: # decrease the maxInt value by factor 10 # as long as the OverflowError occurs. decrement = False try: csv.field_size_limit(maxInt) except OverflowError: maxInt = int(maxInt / 10) decrement = Truewith open(&#x27;test.csv&#x27;, &#x27;rt&#x27;, encoding=&#x27;utf-8&#x27;) as f: fc = csv.DictReader(f) # do something with fc","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"}]},{"title":"Centos 安装 PhantomJS","slug":"centos-install-phantomjs","date":"2018-11-21T08:10:23.000Z","updated":"2020-05-29T09:07:54.413Z","comments":true,"path":"2018/11/21/centos-install-phantomjs/","link":"","permalink":"https://fuyunliu.github.io/2018/11/21/centos-install-phantomjs/","excerpt":"PhantomJS 已经不再开发了，Seleniumn 也警告使用 PhantomJS 是过时的，推荐使用 headless 版的 Chrome 或者 Firefox，但是有时候需要用到，够用就行，而且在 Linux 下安装也相对简单。","text":"PhantomJS 已经不再开发了，Seleniumn 也警告使用 PhantomJS 是过时的，推荐使用 headless 版的 Chrome 或者 Firefox，但是有时候需要用到，够用就行，而且在 Linux 下安装也相对简单。 安装 fontconfig 依赖1yum install -y fontconfig freetype freetype-devel fontconfig-devel libstdc++ 下载 PhantomJS 并解压1234567891011121314151617# 安装到此目录cd /usr/local# 下载wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2# 解压tar -jxvf phantomjs-2.1.1-linux-x86_64.tar.bz2# 重命名mv phantomjs-2.1.1-linux-x86_64 phantomjs# 添加软链接ln -s /usr/local/phantomjs/bin/phantomjs /usr/bin/phantomjs# 验证phantomjs --version 用 Sselenium 驱动 PhantomJS12345678910from selenium import webdriverfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities# 更改浏览器头dcap = dict(DesiredCapabilities.PHANTOMJS)dcap[&quot;phantomjs.page.settings.userAgent&quot;] = &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36&quot;driver = webdriver.PhantomJS(desired_capabilities=dcap)driver.set_page_load_timeout(10)driver.set_script_timeout(10)driver.get(&quot;https://www.baidu.com&quot;) 推荐使用的 Chrome 用法1234567891011121314151617181920212223242526from selenium import webdriverfrom selenium.webdriver.chrome.options import Options# 无界面浏览器options = Options()options.add_argument(&#x27;headless&#x27;)options.add_argument(&#x27;disable-gpu&#x27;)options.add_argument(&#x27;window-size=1200x600&#x27;)# 禁用 javascriptprefs = &#123;&#x27;profile.managed_default_content_settings.javascript&#x27;: 2&#125;options.add_experimental_option(&quot;prefs&quot;, prefs)# 禁止弹出式窗口prefs = &#123;&quot;profile.default_content_setting_values.notifications&quot;: 2&#125;options.add_experimental_option(&quot;prefs&quot;, prefs)# 禁用图片prefs = &#123;&#x27;profile.managed_default_content_settings.images&#x27;: 2&#125;options.add_experimental_option(&quot;prefs&quot;, prefs)driver = webdriver.Chrome(chrome_options=options)# 执行JSdriver.execute_script(&#x27;window.scrollTo(0, 0)&#x27;) # scroll to topdriver.execute_script(&#x27;window.scrollTo(0, document.body.scrollHeight)&#x27;) # end","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"},{"name":"CentOS","slug":"CentOS","permalink":"https://fuyunliu.github.io/tags/CentOS/"}]},{"title":"Python 通过 Thrift 操作 Hbase","slug":"hbase-thrift","date":"2018-11-14T09:04:20.000Z","updated":"2020-05-29T09:09:05.236Z","comments":true,"path":"2018/11/14/hbase-thrift/","link":"","permalink":"https://fuyunliu.github.io/2018/11/14/hbase-thrift/","excerpt":"记录 Python 通过 Thrift 操作 Hbase 的通用操作方法。","text":"记录 Python 通过 Thrift 操作 Hbase 的通用操作方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160from thrift.transport import TSocketfrom thrift.protocol import TBinaryProtocolfrom thrift.transport import TTransportfrom elasticsearch import Elasticsearchfrom hbase import Hbase# Connect to HBase Thrift servertransport = TTransport.TBufferedTransport(TSocket.TSocket(&#x27;localhost&#x27;, 9090))protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)# Create and open the client connectionclient = Hbase.Client(protocol)transport.open()# Connect to Elasticsearch serveres = Elasticsearch(&#x27;localhost&#x27;, http_auth=(&#x27;username&#x27;, &#x27;password&#x27;), port=&#x27;9200&#x27;, timeout=30, max_retries=10, retry_on_timeout=True )def fetch_one(index, doc_type, body, size=1): &quot;&quot;&quot;查询es获取第一条匹配的数据 Arguments: index &#123;str&#125; -- 索引 doc_type &#123;str&#125; -- 类型 body &#123;dict&#125; -- 查询语句 Keyword Arguments: size &#123;int&#125; -- 返回数量 (default: &#123;1&#125;) Returns: dict -- 一条数据，没有结果返回 None &quot;&quot;&quot; res = es.search(index=index, doc_type=doc_type, scroll=&#x27;2m&#x27;, body=body, size=size) hits = res[&#x27;hits&#x27;][&#x27;hits&#x27;] return hits[0] if hits else Nonedef fetch_all(index, doc_type, body, size=100): &quot;&quot;&quot;查询es获取所有匹配的结果 Arguments: index &#123;str&#125; -- 索引 doc_type &#123;str&#125; -- 类型 body &#123;dict&#125; -- 查询语句 Keyword Arguments: size &#123;int&#125; -- 返回数量 (default: &#123;100&#125;) Returns: list -- 结果集 &quot;&quot;&quot; res = es.search(index=index, doc_type=doc_type, scroll=&#x27;2m&#x27;, body=body, size=size) return res[&#x27;hits&#x27;][&#x27;hits&#x27;]def build_term(field, value): &quot;&quot;&quot;term Arguments: field &#123;str&#125; -- 字段 value &#123;str&#125; -- 值 Returns: dict -- 查询语句 &quot;&quot;&quot; body = &#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; field: &#123; &quot;value&quot;: value &#125; &#125; &#125; &#125; return bodydef build_terms(field, values): &quot;&quot;&quot;terms Arguments: field &#123;str&#125; -- 字段 values &#123;list&#125; -- 列表 Returns: dict -- 查询语句 &quot;&quot;&quot; body = &#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; field: values &#125; &#125; &#125; return bodydef get_row_with_columns(table_name, rowkey, columns): &quot;&quot;&quot;根据 rowkey 从 hbase 获取一条数据 Arguments: table_name &#123;str&#125; -- 表名 rowkey &#123;str&#125; -- rowkey attributes &#123;list&#125; -- 属性列表 Returns: dict -- 一条数据，没有则返回None &quot;&quot;&quot; table_name = table_name.encode() rowkey = rowkey.encode() columns = [(&#x27;0:&#x27; + c).encode() for c in columns] res = client.getRowWithColumns(table_name, rowkey, columns, None) if not res: return None d = &#123; k.decode().split(&#x27;:&#x27;)[1]: v.value.decode() for k, v in res[0].columns.items() &#125; d[&#x27;rowkey&#x27;] = res[0].row.decode() return ddef get_rows_with_columns(table_name, rowkeys, columns): &quot;&quot;&quot;根据 rowkeys 从 hbase 获取所有匹配的数据 Arguments: table_name &#123;str&#125; -- 表名 rowkeys &#123;list&#125; -- rowkey 列表 columns &#123;list&#125; -- 指定返回字段 Returns: list -- 数据结果集 &quot;&quot;&quot; data = [] table_name = table_name.encode() rowkeys = [k.encode() for k in rowkeys] columns = [(&#x27;0:&#x27; + c).encode() for c in columns] res = client.getRowsWithColumns(table_name, rowkeys, columns, None) for r in res: d = &#123; k.decode().split(&#x27;:&#x27;)[1]: v.value.decode() for k, v in r.columns.items() &#125; d[&#x27;rowkey&#x27;] = r.row.decode() data.append(d) return data","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"}]},{"title":"Asyncio 笔记","slug":"asyncio-tutorial","date":"2018-11-11T07:02:28.000Z","updated":"2019-08-06T13:58:49.803Z","comments":true,"path":"2018/11/11/asyncio-tutorial/","link":"","permalink":"https://fuyunliu.github.io/2018/11/11/asyncio-tutorial/","excerpt":"并发是指一次处理多件事。 并行是指一次做多件事。 二者不同，但是有联系。 一个关于结构，一个关于执行。 并发用于制定方案，用来解决可能（但未必）并行的问题。 ——Rob Pike Go 语言的创造者之一","text":"并发是指一次处理多件事。 并行是指一次做多件事。 二者不同，但是有联系。 一个关于结构，一个关于执行。 并发用于制定方案，用来解决可能（但未必）并行的问题。 ——Rob Pike Go 语言的创造者之一 异步版 hello-world12345678910111213import asyncioasync def main(): print(&#x27;hello&#x27;) await asyncio.sleep(.1) print(&#x27;world&#x27;)# python3.7 提供asyncio.run(main())# main 函数是个协程，直接运行不会执行操作# main() --&gt; &lt;coroutine object main at 0x109be6d48&gt; 运行协程的三种方式 asyncio.run() 使用 await 关键字 123456789101112131415161718import asyncioimport timeasync def say_after(delay, what): await asyncio.sleep(delay) print(what)async def main(): print(f&quot;started at &#123;time.strftime(&#x27;%X&#x27;)&#125;&quot;) await say_after(1, &#x27;hello&#x27;) await say_after(2, &#x27;world&#x27;) print(f&quot;finished at &#123;time.strftime(&#x27;%X&#x27;)&#125;&quot;)asyncio.run(main()) 使用 asyncio.create_task() 创建一个 Task 对象 1234567891011# 直接对上面的 main 函数进行修改async def main(): t1 = asyncio.create_task(say_after(1, &#x27;hello&#x27;)) t2 = asyncio.create_task(say_after(2, &#x27;world&#x27;)) print(f&quot;started at &#123;time.strftime(&#x27;%X&#x27;)&#125;&quot;) await t1 await t2 print(f&quot;finished at &#123;time.strftime(&#x27;%X&#x27;)&#125;&quot;) Awaitable 对象awaitable 对象是指可以在 await 表达式中使用的对象。 coroutines, Tasks 和 Futures 是 awaitable 对象。 协程 coroutines 1234567891011121314151617import asyncioasync def nested(): return 42async def main(): # 这中方式不会执行 nested 函数 nested() # await print(await nested())asyncio.run(main()) 任务 Tasks 12345678910111213import asyncioasync def nested(): return 42async def main(): t = asyncio.create_task(nested()) await tasyncio.run(main()) 期物 Futures 并发执行 Tasks使用 asyncio.gather 并发执行 Tasks 123456789101112131415161718192021import asyncioasync def factorial(name, number): f = 1 for i in range(2, number + 1): print(f&quot;Task &#123;name&#125;: Compute factorial(&#123;i&#125;)...&quot;) await asyncio.sleep(1) f *= i print(f&quot;Task &#123;name&#125;: factorial(&#123;number&#125;) = &#123;f&#125;&quot;)async def main(): await asyncio.gather( factorial(&#x27;A&#x27;, 2), factorial(&#x27;B&#x27;, 3), factorial(&#x27;C&#x27;, 4), )asyncio.run(main()) 线程和协程的对比12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 线程版以动画形式显示文本旋转指针import threadingimport itertoolsimport timeimport sysclass Signal: go = Truedef spin(msg, signal): write, flush = sys.stdout.write, sys.stdout.flush for char in itertools.cycle(&#x27;|/-\\\\&#x27;): status = char + &#x27; &#x27; + msg write(status) flush() write(&#x27;\\x08&#x27; * len(status)) time.sleep(.1) if not signal.go: break write(&#x27; &#x27; * len(status) + &#x27;\\x08&#x27; * len(status))def slow_funtion(): time.sleep(3) return 42def supervisor(): signal = Signal() spinner = threading.Thread( target=spin, args=(&#x27;thinking!&#x27;, signal)) print(&#x27;spinner object: &#x27;, spinner) spinner.start() result = slow_funtion() signal.go = False spinner.join() return resultdef main(): result = supervisor() print(&#x27;Answer: &#x27;, result)if __name__ == &#x27;__main__&#x27;: main() 123456789101112131415161718192021222324252627282930313233343536373839404142# 协程版以动画形式显示文本旋转指针import asyncioimport itertoolsimport sysasync def spin(msg): write, flush = sys.stdout.write, sys.stdout.flush for char in itertools.cycle(&#x27;|/-\\\\&#x27;): status = char + &#x27; &#x27; + msg write(status) flush() write(&#x27;\\x08&#x27; * len(status)) try: await asyncio.sleep(.1) except asyncio.CancelledError: break write(&#x27; &#x27; * len(status) + &#x27;\\x08&#x27; * len(status))async def slow_funtion(): await asyncio.sleep(3) return 42async def supervisor(): spinner = asyncio.create_task(spin(&#x27;thinking!&#x27;)) print(&#x27;spinner object: &#x27;, spinner) result = await slow_funtion() spinner.cancel() return result# 一般执行方式loop = asyncio.get_event_loop()result = loop.run_until_complete(supervisor())loop.close()print(&#x27;Answer: &#x27;, result)# python3.7 执行方式asyncio.run(supervisor()) Task 对象像是实现协作式多任务的库（如 gevent）中的绿色线程（green thread）。 Task 对象用于驱动协程，Thread 对象用于调用可调用对象。 Task 对象不由自己手动实例化，而是由 asyncio.create_task 方法获取。 获取的 Task 对象已经排定了运行时间，而 Thread 实例需要调用 start 方法运行。 异步版 slow_funtion 是协程，由 await （就是 yield from）驱动。 终止线程需要借助外部变量 go,终止 Task 可以调用 Task.cancel() 实例方法，在协程内部抛出 CancelledError 异常，协程内部也可以捕获这个异常，处理终止请求。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"}]},{"title":"外国小说鉴赏辞典","slug":"foreign-novel-appreciation-dictionary","date":"2018-11-03T02:03:31.000Z","updated":"2019-08-06T14:02:44.978Z","comments":true,"path":"2018/11/03/foreign-novel-appreciation-dictionary/","link":"","permalink":"https://fuyunliu.github.io/2018/11/03/foreign-novel-appreciation-dictionary/","excerpt":"外国小说鉴赏辞典，从图书馆拍来的目录，纯手码，很累！","text":"外国小说鉴赏辞典，从图书馆拍来的目录，纯手码，很累！ 古代至19世纪中期卷 书名 国籍 作者 阿玛莉亚 阿根廷 马莫尔 魂归故里 波兰 克拉舍夫斯基 春香传 朝鲜 佚名 痴儿西木传 德国 格里美尔斯豪森 少年维特的烦恼 德国 歌德 威廉·麦斯特的学习时代 德国 歌德 亲和力 德国 歌德 金罐 德国 霍夫曼 侏儒查赫斯 德国 霍夫曼 雄猫穆尔的生活观 德国 霍夫曼 温亭娜 德国 富凯 彼得·史勒密尔的奇怪故事 德国 沙米索 茵梦湖 德国 施托姆 驿站长 俄国 普希金 黑桃皇后 俄国 普希金 上尉的女儿 俄国 普希金 狂人日记 俄国 果戈里 塔拉斯·布利巴 俄国 果戈里 外套 俄国 果戈里 死魂灵 俄国 果戈里 谁之罪 俄国 赫尔岑 偷东西的喜鹊 俄国 赫尔岑 平凡的故事 俄国 冈察洛夫 当代英雄 俄国 莱蒙托夫 木木 俄国 屠格涅夫 白夜 俄国 陀思妥耶夫斯基 巨人传 法国 拉伯雷 吉尔·布拉斯 法国 勒萨日 波斯人信札 法国 孟德斯鸠 查第格 法国 伏尔泰 老实人 法国 伏尔泰 玛农·列斯戈 法国 普莱服神甫 新爱洛伊丝 法国 卢梭 爱弥儿 法国 卢梭 拉摩的侄子 法国 狄德罗 定命论者雅克和他的主人 法国 狄德罗 阿达拉 法国 夏多布里昂 阿尔芒丝 法国 司汤达 红与黑 法国 司汤达 巴马修道院 法国 司汤达 驴皮记 法国 巴尔扎克 欧也妮·葛朗台 法国 巴尔扎克 高老头 法国 巴尔扎克 幻灭 法国 巴尔扎克 邦斯舅舅 法国 巴尔扎克 三个火枪手 法国 大仲马 基督山恩仇记 法国 大仲马 巴黎圣母院 法国 雨果 高龙巴 法国 梅里美 嘉尔曼 法国 梅里美 巴黎的秘密 法国 欧仁·苏 安吉堡的磨工 法国 乔治·桑 一个世纪的忏悔 法国 缪塞 外祖母 捷克 涅姆佐娃 癞皮鹦鹉 墨西哥 利萨尔迪 瑞普·凡·温克尔 美国 欧文 睡谷的传说 美国 欧文 最后的莫西干人 美国 库柏 杀鹿人 美国 库柏 拉帕其尼医生的女儿 美国 霍桑 红字 美国 霍桑 七个尖角顶的宅第 美国 霍桑 黑猫 美国 爱伦·坡 厄舍府的倒塌 美国 爱伦·坡 毛格街血案 美国 爱伦·坡 汤姆叔叔的小屋 美国 斯托夫人 白鲸 美国 麦尔维尔 竹取物语 日本 佚名 源氏物语 日本 紫式部 平家物语 日本 佚名 浮世澡堂 日本 式亭三马 绿衣亨利 瑞士 凯勒 小癞子 西班牙 佚名 堂吉诃德 西班牙 塞万提斯 金驴记 古罗马 阿普列乌斯 十日谈 意大利 卜伽丘 太阳城 意大利 康帕内拉 约婚夫妇 意大利 孟佐尼 坎特伯雷故事集 英国 乔叟 天路历程 英国 班扬 鲁宾逊漂流记 英国 笛福 格列佛游记 英国 斯威夫特 帕梅拉 英国 理查森 大伟人江奈生·魏尔德传 英国 菲尔丁 汤姆·琼斯 英国 菲尔丁 感伤的旅程 英国 斯特恩 蓝登传 英国 斯摩莱特 威克菲牧师传 英国 哥尔斯密 爱丁堡监狱 英国 司各特 艾凡赫 英国 司各特 理智与情感 英国 奥斯丁 傲慢与偏见 英国 奥斯丁 爱玛 英国 奥斯丁 玛丽·巴顿 英国 盖斯凯尔夫人 名利场 英国 萨克雷 钮可谟一家 英国 李敦 庞贝城的末日 英国 狄更斯 匹克维克外传 英国 狄更斯 雾都孤儿 英国 狄更斯 老古玩店 英国 狄更斯 大卫·考坡菲 英国 狄更斯 简·爱 英国 夏洛蒂·勃朗特 呼啸山庄 英国 艾米莉·勃朗特 19世纪下半期卷 书名 国籍 作者 为了面包 波兰 显克微支 火与剑 波兰 显克微支 洪流 波兰 显克微支 十字军骑士 波兰 显克微支 傀儡 波兰 普鲁斯 福地 波兰 莱蒙特 三色紫罗兰 德国 史托姆 白马骑士 德国 史托姆 艾菲·布里斯特 德国 冯塔纳 奥勃洛摩夫 俄国 冈察洛夫 悬崖 俄国 冈察洛夫 罗亭 俄国 屠格涅夫 贵族之家 俄国 屠格涅夫 前夜 俄国 屠格涅夫 初恋 俄国 屠格涅夫 父与子 俄国 屠格涅夫 罪与罚 俄国 陀思妥耶夫斯基 白痴 俄国 陀思妥耶夫斯基 群魔 俄国 陀思妥耶夫斯基 卡拉马佐夫兄弟 俄国 陀思妥耶夫斯基 怎么办？ 俄国 车尔尼雪夫斯基 琉森 俄国 列夫·托尔斯泰 哥萨克 俄国 列夫·托尔斯泰 战争与和平 俄国 列夫·托尔斯泰 安娜·卡列尼娜 俄国 列夫·托尔斯泰 伊万·伊利奇之死 俄国 列夫·托尔斯泰 复活 俄国 列夫·托尔斯泰 大堂神父 俄国 列斯科夫 左撇子 俄国 列斯科夫 盲音乐家 俄国 柯罗连科 棕榈 俄国 迦尔洵 变色龙 俄国 契诃夫 苦恼 俄国 契诃夫 草原 俄国 契诃夫 第六病室 俄国 契诃夫 装在套子里的人 俄国 契诃夫 悲惨世界 法国 雨果 海上劳工 法国 雨果 笑面人 法国 雨果 九三年 法国 雨果 包法利夫人 法国 福楼拜 萨朗波 法国 福楼拜 情感教育 法国 福楼拜 格兰特船长的儿女 法国 凡尔纳 起义者 法国 瓦莱斯 小酒店 法国 左拉 萌芽 法国 左拉 金钱 法国 左拉 小东西 法国 都德 最后一课 法国 都德 苔依丝 法国 法郎士 羊脂球 法国 莫泊桑 一生 法国 莫泊桑 我的叔叔于勒 法国 莫泊桑 项链 法国 莫泊桑 漂亮朋友 法国 莫泊桑 起义者 菲律宾 黎萨尔 玛丽亚 哥伦比亚 伊萨克斯 马格斯·哈弗拉尔 荷兰 穆尔塔图里 野姑娘芭拉 捷克 聂姆佐娃 庄园内外 捷克 聂姆佐娃 竞选州长 美国 马克·吐温 汤姆·索亚历险记 美国 马克·吐温 王子与贫儿 美国 马克·吐温 哈克贝里·芬历险记 美国 马克·吐温 塞拉斯·拉帕姆的发迹 美国 豪威尔斯 黛西·米勒 美国 詹姆斯 一位女士的画像 美国 詹姆斯 嘉莉妹妹 美国 德莱塞 饥饿 挪威 汉姆生 阿马罗神父的罪恶 葡萄牙 克罗兹 舞姬 日本 森鸥外 浮云 日本 二叶亭四迷 慈悲心肠 西班牙 佩雷斯·加尔多斯 庭长夫人 西班牙 克拉林 金人 匈牙利 约卡伊·莫尔 圣彼得的伞 匈牙利 米克沙特·卡尔曼 奇婚记 匈牙利 米克沙特·卡尔曼 斯巴达克思 意大利 乔万尼奥里 女乞丐 印度 泰戈尔 饥饿的石头 印度 泰戈尔 妻子和女儿 英国 盖斯凯尔夫人 艰难时世 英国 狄更斯 双城记 英国 狄更斯 远大前程 英国 狄更斯 教师 英国 夏洛蒂·勃朗特 亚当·贝德 英国 乔治·艾略特 佛洛斯河磨坊 英国 乔治·艾略特 织工马南转 英国 乔治·艾略特 米德尔马契 英国 乔治·艾略特 远离尘嚣 英国 哈代 还乡 英国 哈代 卡斯特桥市长 英国 哈代 德伯家的苔丝 英国 哈代 无名的裘德 英国 哈代 化身博士 英国 斯蒂文森 诱拐 英国 斯蒂文森 金银岛 英国 斯蒂文森 道林·格雷的画像 英国 王尔德 黑暗的心 英国 康拉德 吉姆爷 英国 康拉德 牛虻 英国 伏尼契 丛林故事 英国 吉卜林 时间机器 英国 威尔斯 马丁·里瓦斯 智利 布莱斯特·加纳 20世纪前期卷 书名 国籍 作者 都柏林人 爱尔兰 乔伊斯 一个青年艺术家的画像 爱尔兰 乔伊斯 尤利西斯 爱尔兰 乔伊斯 一个陌生女人的来信 奥地利 茨威格 变形记 奥地利 卡夫卡 饥饿艺术家 奥地利 卡夫卡 城堡 奥地利 卡夫卡 地洞 奥地利 卡夫卡 农民 波兰 莱蒙特 征服者贝莱 丹麦 尼克索 垃圾教授 德国 亨利希·曼 臣仆 德国 亨利希·曼 布登勃洛克一家 德国 托马斯·曼 死于威尼斯 德国 托马斯·曼 魔山 德国 托马斯·曼 荒原狼 德国 黑塞 纳尔齐斯与歌尔德蒙 德国 黑塞 柏林，亚历山大广场 德国 德布林 西线无战事 德国 雷马克 《基督与反基督》三部曲 俄国 梅列日科夫斯基 决斗 俄国 库普林 石榴石手镯 俄国 库普林 乡村 俄国 布宁 阿尔谢尼耶夫的一生 俄国 布宁 红笑 俄国 安德烈耶夫 七个被绞死者的故事 俄国 安德烈耶夫 企鹅岛 法国 法郎士 诸神渴了 法国 法郎士 约翰·克里斯朵夫 法国 罗曼·罗兰 母与子 法国 罗曼·罗兰 伪币制造者 法国 纪德 追寻逝去的时光 法国 普鲁斯特 火线-一个步兵班的日记 法国 巴比塞 苔蕾丝·德斯盖鲁 法国 莫里亚克 旋涡 哥伦比亚 里维拉 好兵帅克 捷克 哈谢克 折断的翅膀 黎巴嫩 纪伯伦 麦琪的礼物 美国 欧·亨利 最后一片叶子 美国 欧·亨利 章鱼 美国 诺里斯 珍尼姑娘 美国 德莱塞 美国悲剧 美国 德莱塞 啊，拓荒者！ 美国 凯瑟 荒野的呼唤 美国 杰克·伦敦 小城畸人 美国 杰克·伦敦 了不起的盖茨比 美国 菲茨杰拉德 喧哗与骚动 美国 福克纳 我弥留之际 美国 福克纳 太阳照常升起 美国 海明威 永别了，武器 美国 海明威 情侣 缅甸 詹姆斯拉觉 劳伦斯之女克里斯丁 挪威 温塞特 我是猫 日本 夏目漱石 棉被 日本 田山花袋 破戒 日本 岛崎藤村 新珠 日本 菊池宽 罗生门 日本 芥川龙之介 橘子 日本 芥川龙之介 竹林中 日本 芥川龙之介 太阳 日本 横光利一 没有太阳的街 日本 德永直 蟹工船 日本 小林多喜二 雅考伯·冯·贡腾 瑞士 罗伯特·瓦尔泽 母亲 苏联 高尔基 奥库罗夫镇 苏联 高尔基 阿尔塔莫诺夫家的事业 苏联 高尔基 彼得堡 苏联 别雷 我们 苏联 扎米亚京 孽卵 苏联 布尔加科夫 狗心 苏联 布尔加科夫 第四十一 苏联 拉夫列尼约夫 基坑 苏联 普拉东诺夫 伊斯坦布尔的姑娘 土耳其 君泰金 堂娜芭芭拉 委内瑞拉 加列戈斯 迷雾 西班牙 乌纳穆诺 碧血黄沙 西班牙 布拉斯科·伊巴涅斯 孤独者 意大利 皮兰德娄 坛子 意大利 皮兰德娄 已故的帕斯卡尔 意大利 皮兰德娄 沉船 印度 泰戈尔 戈拉 印度 泰戈尔 斯里甘特 印度 查特吉 仁爱道院 印度 普列姆昌德 半斤小麦 印度 普列姆昌德 有产业的人 英国 高尔斯华绥 穿破裤子的慈善家 英国 特莱赛尔 人生的枷锁 英国 毛姆 看得见风景房间 英国 福斯特 印度之行 英国 福斯特 密林中的村庄 英国 伦纳德·伍尔夫 达洛卫夫人 英国 弗吉尼亚·伍尔夫 到灯塔去 英国 弗吉尼亚·伍尔夫 儿子与情人 英国 劳伦斯 虹 英国 劳伦斯 查泰莱夫人的情人 英国 劳伦斯 20世纪中期卷 书名 国籍 作者 小径分岔的花园 阿根廷 博尔赫斯 环形废墟 阿根廷 博尔赫斯 人树 澳大利亚 怀特 一杯茶 澳大利亚 怀特 象棋的故事 奥地利 茨威格 无边的土地 巴西 亚马多 绿蒂在魏玛 德国 托马斯·曼 玻璃球游戏 德国 黑塞 戈雅 德国 福伊希特万格 凯旋门 德国 雷马克 第七个十字架 德国 西格斯 迷惘 德国 卡内蒂 小丑之见 德国 伯尔 淡漠的人 德国 伦茨 铁皮鼓 德国 格拉斯 猫与鼠 德国 格拉斯 分裂的天空 德国 沃尔夫 俄罗斯森林 俄罗斯 列昂诺夫 伊万·杰尼索维奇的一天 俄罗斯 索尔仁尼琴 癌病房 俄罗斯 索尔仁尼琴 蒂博一家 法国 马丁·杜·加尔 蛇结 法国 莫里亚克 小王子 法国 圣埃克絮佩里 人的命运 法国 马尔罗 法兰西组曲 法国 内米洛夫斯基 恶心 法国 萨特 墙 法国 萨特 局外人 法国 加缪 鼠疫 法国 加缪 弗兰德公路 法国 西蒙 琴声如诉 法国 杜拉斯 橡皮 法国 罗伯-格里耶 窥视者 法国 罗伯-格里耶 变 法国 布托尔 你好，忧愁 法国 萨冈 伊萨贝尔在马孔多观雨时的独白 哥伦比亚 马尔克斯 百年孤独 哥伦比亚 马尔克斯 消失了的足迹 古巴 卡彭铁尔 查密莉雅 吉尔吉斯斯坦 艾特玛托夫 愚人船 美国 波特 北回归线 美国 米勒 夜色温柔 美国 菲茨杰拉德 八月之光 美国 福克纳 押沙龙，押沙龙！ 美国 福克纳 熊 美国 福克纳 乞力马扎罗的雪 美国 海明威 老人与海 美国 海明威 洛丽塔 美国 博纳科夫 市场街的斯宾诺莎 美国 辛格 店员 美国 马拉默德 魔桶 美国 马拉默德 赫索格 美国 贝娄 伤心咖啡馆之歌 美国 麦卡勒斯 麦田里的守望者 美国 塞林格 在路上 美国 凯鲁亚克 五号屠场 美国 冯内古特 第二十二条军规 美国 海勒 裸者与死者 美国 梅勒 好人难寻 美国 奥康纳 他们 美国 奥茨 城市与狗 秘鲁 略萨 最明净的地区 墨西哥 富恩特斯 诺言 瑞士 迪伦马特 细雪 日本 谷崎润一郎 上海 日本 横光利一 来到农村的文工队 日本 德永直 雪国 日本 川端康成 睡美人 日本 川端康成 “帝国银行事件”之谜 日本 松本清账 西阵之蝶 日本 水上勉 骏河夫人 日本 司马辽太郎 潮骚 日本 三岛由纪夫 金阁寺 日本 三岛由纪夫 黑衣 日本 有吉佐和子 饲育 日本 大江健三郎 万延元年的足球 日本 大江健三郎 苦难的历程 苏联 阿列克赛·托尔斯泰 日瓦戈医生 苏联 帕斯捷尔纳克 大师和玛格丽特 苏联 布尔加科夫 静静的顿河 苏联 肖洛霍夫 一个人的遭遇 苏联 肖洛霍夫 基督的最后诱惑 希腊 卡赞扎基斯 房间与街道 意大利 摩拉维亚 分成两半的子爵 意大利 卡尔维诺 刀锋 英国 毛姆 海浪 英国 伍尔夫 美妙的新世界 英国 赫胥黎 城堡 英国 克罗宁 一九八四 英国 奥威尔 问题的核心 英国 格林 蝇王 英国 戈尔丁 发条橙 英国 伯吉斯 沙堡 英国 默多克 金色笔记 英国 莱辛 幸运的吉姆 英国 艾米斯 法国中尉的女人 英国 福尔斯 戈丹 印度 普列姆昌德 黑水洋彼岸 印度 安纳德 20世纪后期卷 书名 国籍 作者 蜘蛛女之吻 阿根廷 普伊格 平民史诗 埃及 马哈福兹 风暴眼 澳大利亚 怀特 美好的美好的时光 奥地利 耶利内克 钢琴教师 奥地利 耶利内克 情欲 奥地利 耶利内克 浪女回归 巴西 亚马多 我坐在彼得拉河畔哭泣 巴西 科埃略 韦罗妮卡决定去死 巴西 科埃略 方尖碑 白俄罗斯 贝科夫 战争中没有女性 白俄罗斯 阿列克茜叶维契 惊马奔逃 德国 瓦尔泽 我的世纪 德国 格拉斯 献词 德国 施特劳斯 香水-一个谋杀犯的故事 德国 聚斯金德 白比姆黑耳朵 俄罗斯 特罗耶波尔斯基 一幅画 俄罗斯 格拉宁 永远十九岁 俄罗斯 巴拉克诺夫 百慕大三角 俄罗斯 邦达列夫 鱼王 俄罗斯 阿斯塔菲耶夫 被取消的演出 俄罗斯 奥库扎瓦 活下去，并且要记住 俄罗斯 拉斯普京 告别马焦拉 俄罗斯 拉斯普京 命运线 俄罗斯 哈里托诺夫 书市上的斯薇特兰娜 俄罗斯 马卡宁 美狄娅和她的孩子们 俄罗斯 乌利茨卡娅 夏伯阳与虚空 俄罗斯 佩列文 乡间的房子 俄罗斯 瓦尔拉莫夫 暗店街 法国 莫迪亚诺 我走了 法国 艾什诺兹 流浪的星星 法国 克莱齐奥 霍乱时期的爱情 哥伦比亚 马尔克斯 乙火 韩国 金东里 一日长于百年 吉尔吉斯斯坦 艾特玛托夫 断头台 吉尔吉斯斯坦 艾特玛托夫 斯通家史札记 加拿大 希尔兹 笑忘录 捷克 昆拉德 不能承受的生命之轻 捷克 昆拉德 冤家，一个爱情故事 美国 辛格 童爱 美国 辛格 杜宾的传记 美国 马拉默德 洪堡的礼物 美国 贝娄 更多的人死于心碎 美国 贝娄 时震 美国 冯内古特 刽子手之歌 美国 梅勒 苏菲的抉择 美国 斯泰伦 情欲艺术家 美国 霍克斯 铁草 美国 肯尼迪 最蓝的眼睛 美国 莫里森 所罗门之歌 美国 莫里森 白雪公主后传 美国 巴塞尔姆 拉格泰姆时代 美国 多克特罗 兔子回家 美国 厄普代克 变形记 美国 厄普代克 反生活 美国 罗斯 人性的污点 美国 罗斯 遗产-一个真实的故事 美国 罗斯 骏马长嘶 美国 麦卡锡 孤独鸽 美国 麦克默特里 葡萄园 美国 品钦 幽灵之家 美国 阿连德 不规则飞行 美国 纳尔逊 紫色 美国 沃克 纽约女人未眠夜 美国 提尔曼 一千英亩 美国 斯迈利 时时刻刻 美国 坎宁安 达·芬奇密码 美国 布朗 神圣的夜晚 摩洛哥 杰伦 帝国轶闻 墨西哥 帕索 七月的人民 南非 戈迪默 耻 南非 库切 修道院纪事 葡萄牙 萨拉马戈 失明症漫记 葡萄牙 萨拉马戈 失乐园 日本 渡边淳一 挪威的森林 日本 村上春树 无限近似于透明的蓝 日本 村上龙 厨房 日本 吉本芭娜娜 印第安的最后夏天 瑞士 谢赛克斯 哈扎尔辞典 塞尔维亚 帕维奇 老人 苏联 特里丰诺夫 红莓 苏联 舒克申 我的名字叫红 土耳其 帕慕克 独裁者的葬礼 委内瑞拉 彼特里 请听清风倾诉 乌拉圭 奥内蒂 为亡灵弹奏 西班牙 塞拉 无命运的人生 匈牙利 凯尔泰斯 蓝山 以色列 沙莱夫 玫瑰的名字 意大利 埃科 阿纳泰的贝壳 意大利 斯戈隆 人性的因素 英国 格林 河湾 英国 奈保尔 抵达之谜 英国 奈保尔 来自无人地带的明信片 英国 钱伯斯 隐之书 英国 拜厄特 时间中的孩子 英国 麦克尤恩 化学 英国 斯威夫特 长日留痕 英国 石黑一雄 卑微的神灵 印度 罗易 夜阑更深 印度尼西亚 维查雅 人世间 印度尼西亚 普拉姆迪亚 旁边的花园 智利 多诺索","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"小说","slug":"小说","permalink":"https://fuyunliu.github.io/tags/%E5%B0%8F%E8%AF%B4/"}]},{"title":"Flask v0.1 源码阅读","slug":"understand-flask-v01","date":"2018-09-27T09:17:36.000Z","updated":"2019-08-06T14:07:55.599Z","comments":true,"path":"2018/09/27/understand-flask-v01/","link":"","permalink":"https://fuyunliu.github.io/2018/09/27/understand-flask-v01/","excerpt":"flask v0.1 是第一个发布的版本，单文件版，v0.4 是 flask 的最后一个单文件版本，文章中 flask 的源码有修改，因为依赖包有更新。","text":"flask v0.1 是第一个发布的版本，单文件版，v0.4 是 flask 的最后一个单文件版本，文章中 flask 的源码有修改，因为依赖包有更新。 导包部分123456789101112131415161718192021222324252627282930313233343536373839404142434445# python2.5 版本加入 with 语句，低于 2.5 需要引入，高于则忽略。from __future__ import with_statementimport osimport sys# 没有用到from threading import local# jinja2 模板引擎from jinja2 import Environment, PackageLoader, FileSystemLoader# Flask 的 Request 和 Response 继承自 werkzeug 的 Request 和 Responsefrom werkzeug.wrappers import Request as RequestBase, Response as ResponseBase# 最后几行 _request_ctx_stack, current_app, request, session, g 用到。from werkzeug.local import LocalStack, LocalProxy# 用于测试请求上下文，在 Flask 类的方法 test_request_context 中调用，已失效。# 最新版 test_request_context 方法调用 flask.testing 的 make_test_environ_builder，最终调用 werkzeug.test 的 EnvironBuilder。from werkzeug import create_environfrom werkzeug.utils import cached_propertyfrom werkzeug.wsgi import SharedDataMiddleware# 路由from werkzeug.routing import Map, Rule# 错误处理from werkzeug.exceptions import HTTPException, InternalServerError# flask 自带的 session 用到from werkzeug.contrib.securecookie import SecureCookie# 没有用到，作为对外接口from werkzeug import abort, redirectfrom jinja2 import Markup, escape# 用于获取应用程序根目录try: import pkg_resources pkg_resources.resource_streamexcept (ImportError, AttributeError): pkg_resources = None Request 和 Responseflask 的 Request 和 Response 继承自 werkzeug 的 Request 和 Response。 如果你想要自定义 Request 和 Response，你可以继承这两个类，然后指定 Flask 的 request_class 和 response_class。 123456789101112131415161718192021222324252627282930313233343536class Request(RequestBase): &quot;&quot;&quot;请求类&quot;&quot;&quot; def __init__(self, environ): RequestBase.__init__(self, environ) # WSGI 环境 self.endpoint = None # 视图函数的键名 self.view_args = None # 视图函数的参数class Response(ResponseBase): &quot;&quot;&quot;响应类&quot;&quot;&quot; default_mimetype = &#x27;text/html&#x27;class _RequestGlobals(object): passclass _RequestContext(object): &quot;&quot;&quot;请求上下文&quot;&quot;&quot; def __init__(self, app, environ): self.app = app self.url_adapter = app.url_map.bind_to_environ(environ) self.request = app.request_class(environ) self.session = app.open_session(self.request) # 带上下文的 session self.g = _RequestGlobals() # 带上下文的 g self.flashes = None def __enter__(self): _request_ctx_stack.push(self) def __exit__(self, exc_type, exc_value, tb): if tb is None or not self.app.debug: _request_ctx_stack.pop() 几个有用的函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061def url_for(endpoint, **values): &quot;&quot;&quot;函数跳转 endpoint: Flask 类有个 view_functions 字典，存储的就是 endpoint 和 视图函数的映射关系，默认 endpoint 是视图函数的名字 values: 路由传过来的参数 &quot;&quot;&quot; return _request_ctx_stack.top.url_adapter.build(endpoint, values)def flash(message): &quot;&quot;&quot; 消息闪现，存储在 session 中，是个列表 &quot;&quot;&quot; session[&#x27;_flashes&#x27;] = (session.get(&#x27;_flashes&#x27;, [])) + [message]def get_flashed_messages(): &quot;&quot;&quot; 把 session 中存储的消息全部 pop 出来并返回 &quot;&quot;&quot; flashes = _request_ctx_stack.top.flashes if flashes is None: _request_ctx_stack.top.flashes = flashes = \\ session.pop(&#x27;_flashes&#x27;, []) return flashesdef render_template(template_name, **context): &quot;&quot;&quot; 从文件渲染模板 &quot;&quot;&quot; current_app.update_template_context(context) return current_app.jinja_env.get_template(template_name).render(context)def render_template_string(source, **context): &quot;&quot;&quot; 从字符串渲染模板 &quot;&quot;&quot; current_app.update_template_context(context) return current_app.jinja_env.from_string(source).render(context)def _default_template_ctx_processor(): &quot;&quot;&quot; 模板处理，使得在所有模板中可以使用 request, session 和 g 三个全局变量 &quot;&quot;&quot; reqctx = _request_ctx_stack.top return dict( request=reqctx.request, session=reqctx.session, g=reqctx.g )def _get_package_path(name): &quot;&quot;&quot;根据名字获取模块的路径&quot;&quot;&quot; try: return os.path.abspath(os.path.dirname(sys.modules[name].__file__)) except (KeyError, AttributeError): return os.getcwd() Flask 类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355class Flask(object): &quot;&quot;&quot; Flask 类实现了一个 WSGI 应用，只要将包或者模块的名字传递给它 一旦创建的时候，它将首先注册视图函数、路由映射、模板配置等等几个重要的对象 传入包的名字是用于解决应用内部资源的引用，具体请查看 open_resource 函数 一般情况下，你只需要这样创建： from flask import Flask app = Flask(__name__) &quot;&quot;&quot; # 请求类 request_class = Request # 响应类 response_class = Response # 静态文件路径，设置为 None 可以禁用 static_path = &#x27;/static&#x27; # 密钥，用于 cookies 签名验证 secret_key = None # 基于 cookie 的 session 的名字 session_cookie_name = &#x27;session&#x27; # 默认会直接传给 jinja2 的 options 参数值 jinja_options = dict( autoescape=True, extensions=[&#x27;jinja2.ext.autoescape&#x27;, &#x27;jinja2.ext.with_&#x27;] ) def __init__(self, package_name): # 调试模式开关，设置 True 以打开调试模式 # 在调试模式下，应用程序出错会有特殊的错误页面以供调试 # 并且服务会监控文件的变化，文件发生变化会重载服务 self.debug = False # 包或者模块的名字，一旦设置好了就不要改动 self.package_name = package_name # 应用程序顶级目录 self.root_path = _get_package_path(self.package_name) # 包含所有注册好的视图函数字典，键是函数的名字，也用于生成 URL # 值就是函数本身，可以用 route 装饰器注册一个函数 self.view_functions = &#123;&#125; # 所有注册好的错误处理函数，键是错误代码，值是处理函数 # 可以用 errorhandler 注册一个错误处理函数 self.error_handlers = &#123;&#125; # 预处理函数，每次请求之前会执行，用 before_request 装饰器注册 self.before_request_funcs = [] # 后处理函数，每次请求完成以后执行，函数会截获响应并且改变它 # 用 after_request 装饰器注册 self.after_request_funcs = [] # 模板上下文处理器，默认有一个处理函数 _default_template_ctx_processor # 默认的函数功能是向模板上下文添加三个对象 request, session, g # 每个函数执行不需要参数，返回值为字典，用于填充模板上下文 self.template_context_processors = [_default_template_ctx_processor] # 路由映射，在 werkzeug.routing.Map self.url_map = Map() # 架起一个静态资源服务，一般用于开发环境，生产环境用 nginx if self.static_path is not None: self.url_map.add(Rule(self.static_path + &#x27;/&lt;filename&gt;&#x27;, build_only=True, endpoint=&#x27;static&#x27;)) if pkg_resources is not None: target = (self.package_name, &#x27;static&#x27;) else: target = os.path.join(self.root_path, &#x27;static&#x27;) self.wsgi_app = SharedDataMiddleware(self.wsgi_app, &#123; self.static_path: target &#125;) # jinja2 模板配置，包括模板目录和默认开启的功能 self.jinja_env = Environment(loader=self.create_jinja_loader(), **self.jinja_options) # 这是两个模板能用到的函数 # url_for 用于根据 endpoint 获取 URL # get_flashed_messages 用于获取消息闪现 self.jinja_env.globals.update( url_for=url_for, get_flashed_messages=get_flashed_messages ) def create_jinja_loader(self): &quot;&quot;&quot; 加载模板目录，默认目录为 templates &quot;&quot;&quot; if pkg_resources is None: return FileSystemLoader(os.path.join(self.root_path, &#x27;templates&#x27;)) return PackageLoader(self.package_name) def update_template_context(self, context): &quot;&quot;&quot; 为模板上下文注入几个常用的变量，比如 request, session, g context 为填充模板上下文的字典 &quot;&quot;&quot; reqctx = _request_ctx_stack.top for func in self.template_context_processors: context.update(func()) def run(self, host=&#x27;localhost&#x27;, port=5000, **options): &quot;&quot;&quot; 运行开发服务器 &quot;&quot;&quot; from werkzeug import run_simple if &#x27;debug&#x27; in options: self.debug = options.pop(&#x27;debug&#x27;) options.setdefault(&#x27;use_reloader&#x27;, self.debug) options.setdefault(&#x27;use_debugger&#x27;, self.debug) return run_simple(host, port, self, **options) def test_client(self): &quot;&quot;&quot; 为应用程序创建一个测试客户端 &quot;&quot;&quot; from werkzeug import Client return Client(self, self.response_class, use_cookies=True) def open_resource(self, resource): &quot;&quot;&quot; 动态加载模块 &quot;&quot;&quot; if pkg_resources is None: return open(os.path.join(self.root_path, resource), &#x27;rb&#x27;) return pkg_resources.resource_stream(self.package_name, resource) def open_session(self, request): &quot;&quot;&quot; 创建一个 session，secret_key 必须设置 基于 werkzeug.contrib.securecookie.SecureCookie &quot;&quot;&quot; key = self.secret_key if key is not None: return SecureCookie.load_cookie(request, self.session_cookie_name, secret_key=key) def save_session(self, session, response): &quot;&quot;&quot; 保存 session &quot;&quot;&quot; if session is not None: session.save_cookie(response, self.session_cookie_name) def add_url_rule(self, rule, endpoint, **options): &quot;&quot;&quot; 创建 URL 和视图函数的映射规则，等同于 route 装饰器 只是 add_url_rule 并没有为视图函数注册一个 endpoint 这一步也就是向 view_functions 字典添加 endpoint: view_func 键值对 以下： @app.route(&#x27;/&#x27;) def index(): pass 等同于： def index(): pass app.add_url_rule(&#x27;index&#x27;, &#x27;/&#x27;) app.view_functions[&#x27;index&#x27;] = index options: 参数选项详见 werkzeug.routing.Rule &quot;&quot;&quot; options[&#x27;endpoint&#x27;] = endpoint options.setdefault(&#x27;methods&#x27;, (&#x27;GET&#x27;,)) self.url_map.add(Rule(rule, **options)) def route(self, rule, **options): &quot;&quot;&quot; 为给定的 URL 注册一个视图函数 用法： @app.route(&#x27;/&#x27;) def index(): return &#x27;Hello World&#x27; 变量部分可以用尖括号(``/user/&lt;username&gt;``)指定，默认接受任何不带斜杆的字符串 变量也可以指定一个转换器，以指定类型的参数： =========== =========================================== int 整数 float 浮点数 path 路径 =========== =========================================== 值得注意的是 Flask 如何处理结尾的斜杆，核心思路是保证每个 URL 唯一： 1、如果配置了一个带结尾斜杆的 URL，用户请求不带结尾斜杆的这个 URL，则跳转到带结尾斜杆的页面。 2、如果配置了一个不带结尾斜杆的 URL，用户请求带结尾斜杆的这个 URL，则触发404错误。 这和 web 服务器处理静态资源 static 的逻辑是一样的 参数： rule: URL 字符串 methods: 允许的请求方法，是个列表，默认只接受 GET 请求和隐式的 HEAD subdomain: 指定子域名 strict_slashes: 上述对结尾斜杆处理的开关 options: 参数选项详见 `werkzeug.routing.Rule` &quot;&quot;&quot; def decorator(f): self.add_url_rule(rule, f.__name__, **options) self.view_functions[f.__name__] = f return f return decorator def errorhandler(self, code): &quot;&quot;&quot; 注册一个错误码处理函数 用法： @app.errorhandler(404) def page_not_found(): return &#x27;This page does not exist&#x27;, 404 等同于： def page_not_found(): return &#x27;This page does not exist&#x27;, 404 app.error_handlers[404] = page_not_found 参数： code: 错误码 &quot;&quot;&quot; def decorator(f): self.error_handlers[code] = f return f return decorator def before_request(self, f): &quot;&quot;&quot;注册一个预处理函数&quot;&quot;&quot; self.before_request_funcs.append(f) return f def after_request(self, f): &quot;&quot;&quot;注册一个后处理函数&quot;&quot;&quot; self.after_request_funcs.append(f) return f def context_processor(self, f): &quot;&quot;&quot;注册一个模板上下文处理函数&quot;&quot;&quot; self.template_context_processors.append(f) return f def match_request(self): &quot;&quot;&quot; 根据当前请求的路由去和url_map匹配，拿到 enpoint 和 view_args endpoint: 端点，是 view_functions 中对应视图函数的key view_args: 视图函数的参数 &quot;&quot;&quot; rv = _request_ctx_stack.top.url_adapter.match() request.endpoint, request.view_args = rv return rv def dispatch_request(self): &quot;&quot;&quot; 首先调用上面的 match_request 方法拿到 endpoint 和 view_args 根据 endpoint 可以从 view_functions 找到对应的视图函数 再传入视图函数的参数 view_args，并返回结果，这个结果只是函数的返回值 并没有包装成响应类 response_class，可以调用 make_response 方法生成响应 如果函数执行失败，则根据错误码调用对应的错误处理函数 &quot;&quot;&quot; try: endpoint, values = self.match_request() return self.view_functions[endpoint](**values) except HTTPException, e: handler = self.error_handlers.get(e.code) if handler is None: return e return handler(e) except Exception, e: handler = self.error_handlers.get(500) if self.debug or handler is None: raise return handler(e) def make_response(self, rv): &quot;&quot;&quot; 将视图函数的返回值包装成一个真实的响应类 response_class 函数的返回值支持以下几种类型： ======================= =========================================== response_class: 响应类本身，原样返回 str: 字符串，创建相应类并返回 unicode: unicode 编码，utf-8编码后创建相应类并返回 tuple: 元组，解包元组传入参数创建响应类并返回 a WSGI function: WSGI 函数 ======================= =========================================== 参数： rv: 视图函数的返回值 &quot;&quot;&quot; if isinstance(rv, self.response_class): return rv # basestring 是 str 和 unicode 的超类，只支持 python2 if isinstance(rv, basestring): return self.response_class(rv) if isinstance(rv, tuple): return self.response_class(*rv) return self.response_class.force_type(rv, request.environ) def preprocess_request(self): &quot;&quot;&quot; 在分发请求之前执行所有的预处理函数，如果预处理函数有返回值不为 None 则返回结果并中断其余的预处理 &quot;&quot;&quot; for func in self.before_request_funcs: rv = func() if rv is not None: return rv def process_response(self, response): &quot;&quot;&quot; 依次传入响应并执行所有的后处理函数，返回新的响应 &quot;&quot;&quot; session = _request_ctx_stack.top.session if session is not None: # 保存 `session` self.save_session(session, response) for handler in self.after_request_funcs: response = handler(response) return response def wsgi_app(self, environ, start_response): &quot;&quot;&quot; WSGI 应用，可以用中间件包装： app.wsgi_app = MyMiddleware(app.wsgi_app) 参数： environ: WSGI 环境，是一个字典，包含了所有请求的信息 start_response: 回调函数 &quot;&quot;&quot; with self.request_context(environ): rv = self.preprocess_request() if rv is None: rv = self.dispatch_request() response = self.make_response(rv) response = self.process_response(response) return response(environ, start_response) def request_context(self, environ): &quot;&quot;&quot; 通过给定的 WSGI 环境创建一个请求上下文，并把它绑定到当前上下文中 必须通过 with 语句使用，因为 request 对象只在请求上下文 也就是 with 语句块中起作用 用法如下： with app.request_context(environ): do_something_with(request) 参数： environ: WSGI 环境 &quot;&quot;&quot; return _RequestContext(self, environ) def test_request_context(self, *args, **kwargs): &quot;&quot;&quot; 测试请求上下文，参数详见 werkzeug.create_environ &quot;&quot;&quot; return self.request_context(create_environ(*args, **kwargs)) def __call__(self, environ, start_response): &quot;&quot;&quot;调用 wsgi_app 方法&quot;&quot;&quot; return self.wsgi_app(environ, start_response) 全局变量12345_request_ctx_stack = LocalStack()current_app = LocalProxy(lambda: _request_ctx_stack.top.app)request = LocalProxy(lambda: _request_ctx_stack.top.request)session = LocalProxy(lambda: _request_ctx_stack.top.session)g = LocalProxy(lambda: _request_ctx_stack.top.g) werkzeug 的 Local，LocalStack 和 LocalProxyFlask 中有两个上下文（Context）： 应用上下文（App Context） 请求上下文（Request Context） 上下文就是函数运行时所需要的外部变量，当我们运行一个简单的求和函数 sum 是不需要外部变量的，而像 Flask 中的视图函数运行需要知道当前的请求的路由、表单和请求方法等等一些信息。 Django 和 Tornado 把视图函数所需要的外部信息封装成一个对象 Request，并把这个对象当作参数传给视图函数，无论视图函数有没有用到，所以编写 Django 的视图函数会到处都见到一个 request 参数。 而 Flask 则使用了类似 Thread Local 的对象，它可以隔离多线程/协程之间的状态，使得多线程/协程像操作一个全局变量一样操作各自的状态而互不影响，原理就是使用当前的线程ID作为命名空间，保存多份字典，每个线程只获取各自线程ID对应的字典。 Flask 并没有使用 Python 标准库的 Thread Local，而是用了 werkzeug 实现的 Local。 Local 和 threading.local 相似，但是 Local 在 Greenlet 可用的情况下优先使用 getcurrent 获取当前线程ID，用以支持 Gevent 调度。 Local 还有一个 __release_local__ 方法，用以释放当前线程存储的状态信息。 LocalStack 是基于 Local 实现的栈结构，可以入栈（push）、出栈（pop）和获取栈顶对象（top）。 LocalProxy 是作为 Local 的一个代理模式，它接受一个 callable 对象，注意参数不是 Local 实例，这个 callable 对象返回的结果才是 Local 实例，所有对于 LocalProxy 对象的操作都会转发到对应的 Local。 上 当 app = Flask(__name__) 实例化一个 Flask App 时，App Context 并没有立即被入栈，LocalStack 栈顶元素是空的，返回 None 值，current_app，request，session 和 g 也是 unbound 状态，此时使用这些对象会引发 RuntimeError，解决方法是手动将 app.app_context() 入栈。 当 Flask 应用被 werkzeug 自带的开发服务器或者 gunicorn 用于生产的这类 WSGI 服务器架起的时候，每一个请求进来之前会自动将请求上下文（Request Context）入栈。 应用上下文（App Context）在 flask v0.1 版本的源码中没有，后面版本引入，应用上下文也会自动入栈，后面待看。 threading.local 使用： Flask 的 App Context 使用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190# 优先使用 Greenlet 的线程IDtry: from greenlet import getcurrent as get_identexcept ImportError: try: from thread import get_ident except ImportError: from _thread import get_identclass Local(object): # 固定属性 __slots__ = (&#x27;__storage__&#x27;, &#x27;__ident_func__&#x27;) def __init__(self): # 数据存储，是一个字典 object.__setattr__(self, &#x27;__storage__&#x27;, &#123;&#125;) # 获取当前线程ID的方法 object.__setattr__(self, &#x27;__ident_func__&#x27;, get_ident) def __iter__(self): &quot;&quot;&quot;以生成器的方式返回字典的所有元素&quot;&quot;&quot; return iter(self.__storage__.items()) def __call__(self, proxy): &quot;&quot;&quot;创建一个 LocalProxy&quot;&quot;&quot; return LocalProxy(self, proxy) def __release_local__(self): &quot;&quot;&quot;清空当前线程/协程所保存的数据&quot;&quot;&quot; self.__storage__.pop(self.__ident_func__(), None) def __getattr__(self, name): &quot;&quot;&quot;属性访问&quot;&quot;&quot; try: return self.__storage__[self.__ident_func__()][name] except KeyError: raise AttributeError(name) def __setattr__(self, name, value): &quot;&quot;&quot;属性设置&quot;&quot;&quot; ident = self.__ident_func__() storage = self.__storage__ try: storage[ident][name] = value except KeyError: storage[ident] = &#123;name: value&#125; def __delattr__(self, name): &quot;&quot;&quot;属性删除&quot;&quot;&quot; try: del self.__storage__[self.__ident_func__()][name] except KeyError: raise AttributeError(name)class LocalStack(object): &quot;&quot;&quot; Local 的栈结构实现 &quot;&quot;&quot; def __init__(self): # Local 实例 self._local = Local() def __release_local__(self): # 释放当前线程的数据 self._local.__release_local__() def _get__ident_func__(self): # 返回获取当前线程ID的函数 return self._local.__ident_func__ def _set__ident_func__(self, value): # 设置获取当前线程ID的函数 object.__setattr__(self._local, &#x27;__ident_func__&#x27;, value) __ident_func__ = property(_get__ident_func__, _set__ident_func__) del _get__ident_func__, _set__ident_func__ def __call__(self): &quot;&quot;&quot;返回一个 LocalProxy 对象，该对象始终指向 LocalStack 实例的栈顶元素&quot;&quot;&quot; def _lookup(): rv = self.top if rv is None: raise RuntimeError(&#x27;object unbound&#x27;) return rv return LocalProxy(_lookup) def push(self, obj): &quot;&quot;&quot;入栈&quot;&quot;&quot; rv = getattr(self._local, &#x27;stack&#x27;, None) if rv is None: self._local.stack = rv = [] rv.append(obj) return rv def pop(self): &quot;&quot;&quot;出栈&quot;&quot;&quot; stack = getattr(self._local, &#x27;stack&#x27;, None) if stack is None: return None elif len(stack) == 1: release_local(self._local) return stack[-1] else: return stack.pop() @property def top(self): &quot;&quot;&quot;获取栈顶元素&quot;&quot;&quot; try: return self._local.stack[-1] except (AttributeError, IndexError): return None@implements_boolclass LocalProxy(object): &quot;&quot;&quot; Local 的代理模式实现，所有对 LocalProxy 对象的操作，包括属性访问、方法调用和二元操作 都会转发到那个 callable 参数返回的 Local 对象上 &quot;&quot;&quot; __slots__ = (&#x27;__local&#x27;, &#x27;__dict__&#x27;, &#x27;__name__&#x27;, &#x27;__wrapped__&#x27;) def __init__(self, local, name=None): # 把 callable 参数绑定到 __local 属性上 object.__setattr__(self, &#x27;_LocalProxy__local&#x27;, local) # 代理名字 object.__setattr__(self, &#x27;__name__&#x27;, name) if callable(local) and not hasattr(local, &#x27;__release_local__&#x27;): # 注意这里的参数 local 仅仅是一个 callable 对象 # 该对象执行返回的结果才是 Local 实例 object.__setattr__(self, &#x27;__wrapped__&#x27;, local) def _get_current_object(self): &quot;&quot;&quot;获取当前 Local 实例&quot;&quot;&quot; if not hasattr(self.__local, &#x27;__release_local__&#x27;): return self.__local() try: return getattr(self.__local, self.__name__) except AttributeError: raise RuntimeError(&#x27;no object bound to %s&#x27; % self.__name__) @property def __dict__(self): try: return self._get_current_object().__dict__ except RuntimeError: raise AttributeError(&#x27;__dict__&#x27;) def __repr__(self): try: obj = self._get_current_object() except RuntimeError: return &#x27;&lt;%s unbound&gt;&#x27; % self.__class__.__name__ return repr(obj) def __bool__(self): try: return bool(self._get_current_object()) except RuntimeError: return False def __unicode__(self): try: return unicode(self._get_current_object()) # noqa except RuntimeError: return repr(self) def __dir__(self): try: return dir(self._get_current_object()) except RuntimeError: return [] def __getattr__(self, name): if name == &#x27;__members__&#x27;: return dir(self._get_current_object()) return getattr(self._get_current_object(), name) def __setitem__(self, key, value): self._get_current_object()[key] = value def __delitem__(self, key): del self._get_current_object()[key] # 下面的源码省略，LocalProxy 重写了所有的魔法方法","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Flask","slug":"Flask","permalink":"https://fuyunliu.github.io/tags/Flask/"}]},{"title":"RDS for MySQL 备份文件恢复到自建数据库","slug":"rds-for-mysql","date":"2018-09-13T03:45:58.000Z","updated":"2020-05-29T09:09:47.960Z","comments":true,"path":"2018/09/13/rds-for-mysql/","link":"","permalink":"https://fuyunliu.github.io/2018/09/13/rds-for-mysql/","excerpt":"云数据库MySQL版使用开源软件Percona Xtrabackup对数据库进行备份，所以您可以使用该软件将云数据库MySQL的备份文件恢复到自建数据库中，本文将介绍详细的操作步骤。","text":"云数据库MySQL版使用开源软件Percona Xtrabackup对数据库进行备份，所以您可以使用该软件将云数据库MySQL的备份文件恢复到自建数据库中，本文将介绍详细的操作步骤。 软件说明 MySQL 5.6.41 Percona XtraBackup 2.2.9 rds_backup_extract.sh 解压数据库备份文件12unzip -P密码 mysql_data_backup.tar.gz.zipbash rds_backup_extract -f mysql_data_backup.tar.gz -C /data/mysql/data 修改配置文件 backup-my.cnf 如下12345678910111213141516171819# This MySQL options file was generated by innobackupex.# The MySQL server[mysqld]innodb_checksum_algorithm=innodb# innodb_log_checksum_algorithm=innodbinnodb_data_file_path=ibdata1:200M:autoextendinnodb_log_files_in_group=2innodb_log_file_size=1572864000# innodb_fast_checksum=false# innodb_page_size=16384# innodb_log_block_size=512innodb_undo_directory=.innodb_undo_tablespaces=0# rds_encrypt_data=false# innodb_encrypt_algorithm=aes_128_ecb 修改文件属主1chown -R mysql:mysql /data/mysql/data 恢复数据文件12chmod 400 /data/mysql/data/backup-my.cnfinnobackupex --defaults-file=/data/mysql/data/backup-my.cnf --apply-log /data/mysql/data 启动数据库并登入验证1mysqld_safe --defaults-file=/data/mysql/data/backup-my.cnf --user=mysql --datadir=/data/mysql/data --skip-grant-tables &amp; 新建用户恢复完成后，表 mysql.user 中是不包含 RDS 中创建的用户，需要新建，新建用户前请执行如下 SQL： 12345delete from mysql.db where user&lt;&gt;&#x27;root&#x27; and char_length(user)&gt;0;delete from mysql.tables_priv where user&lt;&gt;&#x27;root&#x27; and char_length(user)&gt;0;flush privileges; 参考链接阿里云数据备份/恢复","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://fuyunliu.github.io/tags/MySQL/"},{"name":"CentOS","slug":"CentOS","permalink":"https://fuyunliu.github.io/tags/CentOS/"}]},{"title":"CentOS 离线安装 MySQL","slug":"centos-install-mysql","date":"2018-09-06T03:47:57.000Z","updated":"2020-05-29T09:06:36.611Z","comments":true,"path":"2018/09/06/centos-install-mysql/","link":"","permalink":"https://fuyunliu.github.io/2018/09/06/centos-install-mysql/","excerpt":"记录一下 CentOS 离线安装 MySQL 并配置多实列主从复制的过程，如果有旧版 Mariadb，先卸载旧版 Mariadb。","text":"记录一下 CentOS 离线安装 MySQL 并配置多实列主从复制的过程，如果有旧版 Mariadb，先卸载旧版 Mariadb。 卸载系统自带的 Mariadb123rpm -qa|grep mariadb # 查询出已安装的 mariadbrpm -e --nodeps filename # 上面列出的所有文件rm -f /etc/my.cnf # 删除配置文件 创建 mysql 用户组12groupadd mysqluseradd -g mysql mysql 下载安装包1wget https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.12-linux-glibc2.12-x86_64.tar.xz 解压文件到目录 /usr/local123456789101112131415cp mysql-8.0.12-linux-glibc2.12-x86_64.tar.xz /usr/local/mysql-8.0.12-linux-glibc2.12-x86_64.tar.xzcd /usr/local# xz 结尾的是经过两层压缩的压缩包# 先解压 xzxz -d your_file_name.tar.xz# 再解压 tartar -xvf your_file_name.tar# 或者直接解压tar xvJf your_file_name.tar.xztar xvJf mysql-8.0.12-linux-glibc2.12-x86_64.tar.xzmv mysql-8.0.12-linux-glibc2.12-x86_64 mysql 配置 /etc/my.cnf1234567891011121314151617181920212223242526272829[mysqld_multi]mysqld = /usr/local/mysql/bin/mysqld_safemysqladmin = /usr/local/mysql/bin/mysqladminuser = root# The MySQL server###############################################################################[mysqld1]port =33306datadir =/data/mysqldata/data1socket =/var/lib/mysql/mysql1.sockpid-file =/var/lib/mysql/mysql1.piduser =mysqlserver_id =33306log_bin =/data/mysqldata/data1/mysql-binlog_bin_index =/data/mysqldata/data1/mysql-bin.indexexpire_logs_days =10 # 按需设置过期时间，表示保留最近10天的日志###############################################################################[mysqld2]port =33307datadir =/data/mysqldata/data2socket =/var/lib/mysql/mysql2.sockpid-file =/var/lib/mysql/mysql2.piduser =mysqlserver_id =33307log_bin =/data/mysqldata/data2/mysql-binlog_bin_index =/data/mysqldata/data2/mysql-bin.indexexpire_logs_days =10 # 按需设置过期时间，表示保留最近10天的日志############################################################################### 初始化数据目录1234567891011121314151617181920212223mkdir /var/lib/mysqlchown -R mysql:mysql /var/lib/mysqlmkdir /data/mysqldatachown -R mysql:mysql /data/mysqldatamkdir /data/mysqldata/data1chown -R mysql:mysql /data/mysqldata/data1mkdir /data/mysqldata/data2chown -R mysql:mysql /data/mysqldata/data2cd /usr/local/mysql/bin# 这种初始化数据库目录的方式过时了./mysql_install_db --basedir=/usr/local/mysql --datadir=/data/mysqldata/data1 --user=mysql./mysql_install_db --basedir=/usr/local/mysql --datadir=/data/mysqldata/data2 --user=mysql# 新的初始化数据库目录方式，会在终端打印一个临时登入密码。./mysqld --initialize --basedir=/usr/local/mysql --datadir=/data/mysqldata/data1 --user=mysql --console./mysqld --initialize --basedir=/usr/local/mysql --datadir=/data/mysqldata/data2 --user=mysql --console 启动实例1234cd /usr/local/mysql/binmysqld_multi start 1mysqld_multi start 2mysqld_multi report 主库创建同步账号1234567891011121314151617181920212223242526272829303132# 用刚才初始化数据库目录生成的临时密码登入./mysql -S /var/lib/mysql/mysql1.sock -p your-password# 如果忘记密码，可以在 my.cnf 的 mysqld 块中添加一行 skip-grant-tables = 1# 可以进行无密码登入，修改成功之后去掉这一行然后重启数据库。# 修改 root 密码USE mysql;UPDATE user SET authentication_string = PASSWORD(&#x27;new-password&#x27;), password_expired = &#x27;N&#x27;, password_last_changed = NOW() WHERE user = &#x27;root&#x27;;FLUSH PRIVILEGES;# 授权账户使得局域网内的机器可以访问数据库GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;new-password&#x27; WITH GRANT OPTION;# 创建一个同步账户CREATE USER &#x27;repl&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;repl-password&#x27;;GRANT REPLICATION SLAVE ON *.* TO &#x27;repl&#x27;@&#x27;%&#x27;;# 查看状态SHOW MASTER STATUS;SHOW BINARY LOGS;# 如果忘记设置日志过期时间，可以进入数据库进行全局设置，并手动清理过期日志# 不要在数据库目录进行删除日志，这样会使得数据库日志索引不一致，导致自动清理失效SET GLOBAL EXPIRE_LOGS_DAYS = 10;FLUSH LOGS; # 触发日志清理，一般是在有新的日志生成的时候触发检查一次。SHOW BINARY LOGS;# 也可以手动删除某个日志之前的所有日志PURGE BINARY LOGS TO &#x27;mysql-bin.000080&#x27;; # 删除 80 之前的日志SHOW BINARY LOGS; 从库配置12345678910111213141516# 修改从库的配置文件server-id =2relay-log =/dbdata/data/relay-logrelay-log-index =/dbdata/data/relay-log.index# 进入数据库执行CHANGE MASTER TOMASTER_HOST=‘master_host_name’, # 主库的主机名MASTER_PORT=port_number # 主库的端口号MASTER_USER=‘replication_user_name’, # 复制的数据库用户名MASTER_PASSWORD=‘replication_password’, # 复制的用户密码MASTER_LOG_FILE=‘recorded_log_file_name’, # 主库的日志文件名MASTER_LOG_POS=recorded_log_position; # 主库的日志文件位置start slave;","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://fuyunliu.github.io/tags/MySQL/"},{"name":"CentOS","slug":"CentOS","permalink":"https://fuyunliu.github.io/tags/CentOS/"}]},{"title":"Selenium 在 Ubuntu 服务器上的使用","slug":"selenium-linux","date":"2018-08-13T02:31:07.000Z","updated":"2020-05-29T09:10:51.218Z","comments":true,"path":"2018/08/13/selenium-linux/","link":"","permalink":"https://fuyunliu.github.io/2018/08/13/selenium-linux/","excerpt":"安装 chrome1234wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -echo &#x27;deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main&#x27; | sudo tee /etc/apt/sources.list.d/google-chrome.listsudo apt-get updatesudo apt-get install google-chrome-stable","text":"安装 chrome1234wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -echo &#x27;deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main&#x27; | sudo tee /etc/apt/sources.list.d/google-chrome.listsudo apt-get updatesudo apt-get install google-chrome-stable 安装 chromedriver1234wget -N https://chromedriver.storage.googleapis.com/2.41/chromedriver_linux64.zipunzip chromedriver_linux64.zipchmod +x chromedrivercp chromedriver /usr/bin/ 安装 Xvfb12345sudo apt-get -y install xvfb gtk2-engines-pixbufsudo apt-get -y install xfonts-cyrillic xfonts-100dpi xfonts-75dpi xfonts-base xfonts-scalable# 截图功能，可选sudo apt-get -y install imagemagick x11-appsXvfb -ac :99 -screen 0 1280x1024x16 &amp; export DISPLAY=:99 测试脚本12345678910from selenium import webdriverchrome_options = webdriver.ChromeOptions()chrome_options.add_argument(&#x27;--headless&#x27;)chrome_options.add_argument(&#x27;--disable-gpu&#x27;)driver = webdriver.Chrome(chrome_options=chrome_options,executable_path=&#x27;/usr/bin/chromedriver&#x27;)driver.get(&quot;https://www.baidu.com&quot;)print(driver.title)driver.quit() 遇到的问题ubuntu server 18.04 虽然内置 python3 版本，但是没有 pip在 /etc/apt/sources.list 添加下列源 1234deb http://cn.archive.ubuntu.com/ubuntu bionic main multiverse restricted universedeb http://cn.archive.ubuntu.com/ubuntu bionic-updates main multiverse restricted universedeb http://cn.archive.ubuntu.com/ubuntu bionic-security main multiverse restricted universedeb http://cn.archive.ubuntu.com/ubuntu bionic-proposed main multiverse restricted universe 12sudo apt-get updatesudo apt-get install python3-pip 再用 pip 安装 selenium1pip3 install selenium","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"}]},{"title":"理解 Python 的关键字 Yield","slug":"understand-yield","date":"2018-03-14T06:17:26.000Z","updated":"2018-10-19T15:58:11.250Z","comments":true,"path":"2018/03/14/understand-yield/","link":"","permalink":"https://fuyunliu.github.io/2018/03/14/understand-yield/","excerpt":"为了理解什么是yield,你必须理解什么是生成器。 在理解生成器之前，让我们先走近迭代。 当你建立了一个列表，你可以逐项地读取这个列表，这叫做一个可迭代对象。","text":"为了理解什么是yield,你必须理解什么是生成器。 在理解生成器之前，让我们先走近迭代。 当你建立了一个列表，你可以逐项地读取这个列表，这叫做一个可迭代对象。 123mylist = [1, 2, 3, 4, 5]for i in mylist: print(i) 所有你可以使用for…in…语法的叫做一个迭代器，列表，字符串，文件等等，你经常使用它们是因为你可以如你所愿的读取其中的元素，但是你把所有的值都存储到了内存中，如果你有大量数据的话这个方式并不是你想要的。 生成器是可以迭代的，但是你只可以读取它一次，因为它并不把所有的值放在内存中，它是实时地生成数据。 123mygenerator = (x * x for x in range(5))for i in mygenerator: print(i) 你不可以再次迭代生成器 1234try: next(mygenerator)except StopIteration: print(&quot;停止迭代&quot;) yield 是一个类似 return 的关键字，只是这个函数返回的是个生成器。 123def create_generator(): for i in range(5): yield i * i 如果函数内部使用 return，则返回 0。 1234mygenerator = create_generator()for i in mygenerator: print(i) 斐波拉契数列 12345def fib(): x, y = 0, 1 while True: x, y = y, x + y yield x 获取斐波拉契数列前10个 12import itertoolslist(itertools.islice(fib(), 10)) 杨辉三角 12345def triangle(): a = [1] while True: yield a a = [sum(i) for i in zip([0] + a, a + [0])] 输出前10行杨辉三角 12import pprintlist(itertools.islice(triangle(), 10)) 控制迭代器的穷尽 12345678910111213141516171819202122232425262728293031323334class Bank(): crisis = False # crisis是危机的意思 def create_atm(self): while not self.crisis: yield &quot;$100&quot;bank = Bank() # 创建一个银行corner_street_atm = bank.create_atm() # 创建一个ATM机print([next(corner_street_atm) for _ in range(5)])bank.crisis = True # 危机来了try: print(next(corner_street_atm))except StopIteration: print(&quot;corner_street_atm: no more money!&quot;)try: wall_street_atm = bank.create_atm() print(next(wall_street_atm))except StopIteration: print(&quot;wall_street_atm: no more money!&quot;)bank.crisis = False # 问题是，即使改变crisis的值，ATM依然是空的try: print(next(corner_street_atm))except StopIteration: print(&quot;crisis is %s, and still no more money!&quot; % bank.crisis)# 重新创建一个ATM机，现在有钱了brand_new_atm = bank.create_atm()print([next(brand_new_atm) for _ in range(5)]) itertools 模块包含了许多特殊的迭代方法 比赛中4匹马可能到达终点的先后顺序的可能情况 12345import pprinthorses = [1, 2, 3, 4]races = itertools.permutations(horses)print(races)pprint.pprint(list(races)) 一个实现了 __iter__ 方法的对象是可迭代的，一个实现了 __next__ 方法的对象是迭代器。 12345678910111213141516class Fibs(): def __init__(self): self.a = 0 self.b = 1 def __next__(self): self.a, self.b = self.b, self.a + self.b return self.a def __iter__(self): return selffibs = Fibs()print([next(fibs) for _ in range(10)])","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"}]},{"title":"八大排序算法的 Python 实现","slug":"8-sort-algorithm","date":"2018-03-14T05:48:20.000Z","updated":"2020-11-26T09:31:27.557Z","comments":true,"path":"2018/03/14/8-sort-algorithm/","link":"","permalink":"https://fuyunliu.github.io/2018/03/14/8-sort-algorithm/","excerpt":"插入排序1234567891011def insert_sort(lists): count = len(lists) for i in range(1, count): key = lists[i] j = i - 1 while j &gt;= 0: if lists[j] &gt; key: lists[j + 1] = lists[j] lists[j] = key j -= 1 return lists","text":"插入排序1234567891011def insert_sort(lists): count = len(lists) for i in range(1, count): key = lists[i] j = i - 1 while j &gt;= 0: if lists[j] &gt; key: lists[j + 1] = lists[j] lists[j] = key j -= 1 return lists 希尔排序123456789101112131415161718def shell_sort(lists): count = len(lists) step = 2 group = count // step while group &gt; 0: for i in range(0, group): j = i + group while j &lt; count: k = j - group key = lists[j] while k &gt;= 0: if lists[k] &gt; key: lists[k + group] = lists[k] lists[k] = key k -= group j += group group //= step return lists 冒泡排序1234567def bubble_sort(lists): count = len(lists) for i in range(0, count): for j in range(i + 1, count): if lists[i] &gt; lists[j]: lists[i], lists[j] = lists[j], lists[i] return lists 快速排序123456789101112131415161718192021qs = lambda xs: ((len(xs) &lt;= 1 and [xs]) or [qs([x for x in xs[1:] if x &lt; xs[ 0]]) + [xs[0]] + qs([x for x in xs[1:] if x &gt;= xs[0]])])[0]def quick_sort(lists, left=0, right=9): if left &gt;= right: return lists key = lists[left] low = left high = right while left &lt; right: while left &lt; right and lists[right] &gt;= key: right -= 1 lists[left] = lists[right] while left &lt; right and lists[left] &lt;= key: left += 1 lists[right] = lists[left] lists[right] = key quick_sort(lists, low, left - 1) quick_sort(lists, left + 1, high) return lists 选择排序123456789def select_sort(lists): count = len(lists) for i in range(0, count): min = i for j in range(i + 1, count): if lists[min] &gt; lists[j]: min = j lists[min], lists[i] = lists[i], lists[min] return lists 堆排序1234567891011121314151617181920212223242526def adjust_heap(lists, i, size): lchild = 2 * i + 1 rchild = 2 * i + 2 max = i if i &lt; size // 2: if lchild &lt; size and lists[lchild] &gt; lists[max]: max = lchild if rchild &lt; size and lists[rchild] &gt; lists[max]: max = rchild if max != i: lists[max], lists[i] = lists[i], lists[max] adjust_heap(lists, max, size)def build_heap(lists, size): for i in range(0, (size // 2))[::-1]: adjust_heap(lists, i, size)def heap_sort(lists): size = len(lists) build_heap(lists, size) for i in range(0, size)[::-1]: lists[0], lists[i] = lists[i], lists[0] adjust_heap(lists, 0, i) return lists 归并排序12345678910111213141516171819202122def merge(left, right): i, j = 0, 0 result = [] while i &lt; len(left) and j &lt; len(right): if left[i] &lt;= right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result += left[i:] result += right[j:] return resultdef merge_sort(lists): if len(lists) &lt;= 1: return lists num = len(lists) // 2 left = merge_sort(lists[:num]) right = merge_sort(lists[num:]) return merge(left, right) 基数排序1234567891011121314import mathdef radix_sort(lists, radix=10): k = int(math.ceil(math.log(max(lists), radix))) bucket = [[] for _ in range(radix)] for i in range(1, k + 1): for j in lists: bucket[j // (radix**(i - 1)) % radix].append(j) del lists[:] for z in bucket: lists += z del z[:] return lists 测试12345678910111213141516171819202122232425262728293031323334353637import randomoriginal_test = list(random.randint(1, 100) for _ in range(10))print(&quot;原始列表： %s&quot; % original_test)# 插入排序insert_test = insert_sort(original_test)# 希尔排序shell_test = shell_sort(original_test)# 冒泡排序bubble_test = bubble_sort(original_test)快速排序quick_test = quick_sort(original_test)# 直接选择排序select_test = select_sort(original_test)# 堆排序heap_test = heap_sort(original_test)# 归并排序merge_test = merge_sort(original_test)# 基数排序radix_test = radix_sort(original_test)print(&quot;插入排序： %s&quot; % insert_test)print(&quot;希尔排序： %s&quot; % shell_test)print(&quot;冒泡排序： %s&quot; % bubble_test)print(&quot;快速排序： %s&quot; % quick_test)print(&quot;直接选择排序：%s&quot; % select_test)print(&quot;堆排序： %s&quot; % heap_test)print(&quot;归并排序： %s&quot; % merge_test)print(&quot;基数排序： %s&quot; % radix_test)print(&quot;快速排序： %s&quot; % qs(original_test)) 本文来自：八大排序算法的 Python 实现","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"}]},{"title":"Screen 使用教程","slug":"screen-usage","date":"2018-03-13T09:36:52.000Z","updated":"2020-05-29T09:10:27.167Z","comments":true,"path":"2018/03/13/screen-usage/","link":"","permalink":"https://fuyunliu.github.io/2018/03/13/screen-usage/","excerpt":"GNU Screen是一款由GNU计划开发的用于命令行终端切换的自由软件。用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换。","text":"GNU Screen是一款由GNU计划开发的用于命令行终端切换的自由软件。用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换。 安装 screen1yum install -y screen screen 常用命令新建一个Screen Session 1screen -S session_name 将当前Screen Session放到后台 1CTRL + A + D 唤起一个Screen Session 1screen -r session_name 分享一个Screen Session 1screen -x session_name 终止一个Screen Session 123exitorCTRL + D 默认显示一屏的内容，要查看之前内容，如下操作： 1Ctrl + A ESC 列表所有的会话 1screen -ls 进入某个会话 1screen -r session_name 如果进不去，则 1screen -d session_name 再 1screen -r session_name ctrl + A + N 切换窗口","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"CentOS","slug":"CentOS","permalink":"https://fuyunliu.github.io/tags/CentOS/"}]},{"title":"几种启动 Flask 应用的方式","slug":"flask-start-up","date":"2018-03-12T02:15:58.000Z","updated":"2019-08-06T14:02:19.167Z","comments":true,"path":"2018/03/12/flask-start-up/","link":"","permalink":"https://fuyunliu.github.io/2018/03/12/flask-start-up/","excerpt":"记录几种启动 Flask 应用的方式","text":"记录几种启动 Flask 应用的方式 首先写一个简单的 index.py 1234567891011# -*- coding: utf-8 -*-from flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;, methods=[&#x27;GET&#x27;])def hello(): return &#x27;hello, world!&#x27; 最简单的启动方式12if __name__ == &#x27;__main__&#x27;: app.run() 这只能用于开发模式，可以设置debug=True开启调试模式，并且这是单线程同步的。 用 tornado 驱动 flask写一个server.py，并将上面index.py中的启动代码去掉，终端运行python server.py。 12345678910# -*- coding: utf-8 -*-from tornado.wsgi import WSGIContainerfrom tornado.httpserver import HTTPServerfrom tornado.ioloop import IOLoopfrom index import apphttp_server = HTTPServer(WSGIContainer(app))http_server.listen(5000) # flask默认的端口IOLoop.instance().start() 这也是同步的，同一时间只能处理一个请求，可以写个简单的接口测试一下。 12345678910111213141516171819202122# -*- coding: utf-8 -*-import timefrom flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;, methods=[&#x27;GET&#x27;])def hello(): return &#x27;hello, world!&#x27;@app.route(&#x27;/test&#x27;, methods=[&#x27;GET&#x27;])def test(): for n in range(10): print(n) time.sleep(2) return &#x27;hello&#x27; 用postman同时发起5个请求，后台按顺序打印0-9，5个请求是一个一个执行的。 用 twisted 驱动 flask这个可以同时处理多个请求。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# -*- coding: utf-8 -*-import timefrom flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;, methods=[&#x27;GET&#x27;])def hello(): return &#x27;hello, world!&#x27;@app.route(&#x27;/test&#x27;, methods=[&#x27;GET&#x27;])def test(): for n in range(10): print(n) time.sleep(2) return &#x27;hello&#x27;if __name__ == &quot;__main__&quot;: reactor_args = &#123;&#125; def run_twisted_wsgi(): from twisted.internet import reactor from twisted.web.server import Site from twisted.web.wsgi import WSGIResource resource = WSGIResource(reactor, reactor.getThreadPool(), app) site = Site(resource) reactor.listenTCP(5000, site) reactor.run(**reactor_args) if app.debug: # Disable twisted signal handlers in development only. reactor_args[&#x27;installSignalHandlers&#x27;] = 0 # Turn on auto reload. import werkzeug.serving run_twisted_wsgi = werkzeug.serving.run_with_reloader(run_twisted_wsgi) run_twisted_wsgi()if __name__ == &#x27;__main__&#x27;: app.run()","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Flask","slug":"Flask","permalink":"https://fuyunliu.github.io/tags/Flask/"}]},{"title":"CentOS 编译安装 Python3","slug":"centos-install-python3","date":"2018-03-12T00:33:42.000Z","updated":"2020-05-29T09:08:04.943Z","comments":true,"path":"2018/03/12/centos-install-python3/","link":"","permalink":"https://fuyunliu.github.io/2018/03/12/centos-install-python3/","excerpt":"记录一下 CentOS 编译安装 Python3 的过程。","text":"记录一下 CentOS 编译安装 Python3 的过程。 安装系统相关依赖1yum install -y zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel openssl-static openssl-devel xz xz-devel libffi-devel findutils gcc wget 下载 python3 包1wget https://www.python.org/ftp/python/3.6.9/Python-3.6.10.tgz 解压到当前目录1tar -zxvf Python-3.6.10.tgz 进入生成的目录进行配置1./configure --prefix=/usr/local/python3 --enable-loadable-sqlite-extensions --enable-optimizations 编译安装1make &amp;&amp; make install 添加软连接1ln -s /usr/local/python3/bin/python3 /usr/bin/python3 其他yum 搜索可用包 1yum search python3 | grep devel 一键更新 python 包 1pip list --outdated --format=freeze | grep -v &#x27;^\\-e&#x27; | cut -d = -f 1 | xargs -n1 pip install -U 切换豆瓣源 12345678# 编辑 .pip/pip.conf 添加如下内容[global]index-url = https://pypi.douban.com/simpletrusted-host = pypi.douban.com[list]format = columns 离线安装python包 12# 离线安装，指定寻找包的目录为 packagespython3 -m pip install --no-index --find-links=packages -r requirements.txt","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"},{"name":"CentOS","slug":"CentOS","permalink":"https://fuyunliu.github.io/tags/CentOS/"}]},{"title":"Python 脚本自动重载","slug":"python-reloader","date":"2018-03-09T09:51:08.000Z","updated":"2020-05-29T09:09:37.895Z","comments":true,"path":"2018/03/09/python-reloader/","link":"","permalink":"https://fuyunliu.github.io/2018/03/09/python-reloader/","excerpt":"Django 和 Flask 应用开启 debug 模式之后都能检测代码的变化然后自动重载，于是去找实现代码，发现 Flask 是用的 werkzeug 库里面的功能，而 Django 的不好用于自己写的脚本，因为和 Django 应用结合了。","text":"Django 和 Flask 应用开启 debug 模式之后都能检测代码的变化然后自动重载，于是去找实现代码，发现 Flask 是用的 werkzeug 库里面的功能，而 Django 的不好用于自己写的脚本，因为和 Django 应用结合了。 下面是 werkzeug 中的 _reloader 模块中的 run_with_reloader 函数。 1234567891011121314151617def run_with_reloader(main_func, extra_files=None, interval=1, reloader_type=&#x27;auto&#x27;): &quot;&quot;&quot;Run the given function in an independent python interpreter.&quot;&quot;&quot; import signal reloader = reloader_loops[reloader_type](extra_files, interval) signal.signal(signal.SIGTERM, lambda *args: sys.exit(0)) try: if os.environ.get(&#x27;WERKZEUG_RUN_MAIN&#x27;) == &#x27;true&#x27;: t = threading.Thread(target=main_func, args=()) t.setDaemon(True) t.start() reloader.run() else: sys.exit(reloader.restart_with_reloader()) except KeyboardInterrupt: pass 用法如下 123456789101112import timedef main(): while True: print(&quot;hello, world!&quot;) time.sleep(2)if __name__ == &#x27;__main__&#x27;: run_with_reloader(main) 但是执行函数不能传参，修改 run_with_reloader 如下 12345678910111213141516171819202122def run_with_reloader(main_func, args=(), kwargs=None, extra_files=None, interval=1, reloader_type=&#x27;auto&#x27;): &quot;&quot;&quot;Run the given function in an independent python interpreter.&quot;&quot;&quot; import os import sys import signal import threading from werkzeug._reloader import reloader_loops reloader = reloader_loops[reloader_type](extra_files, interval) signal.signal(signal.SIGTERM, lambda *args: sys.exit(0)) try: if os.environ.get(&#x27;WERKZEUG_RUN_MAIN&#x27;) == &#x27;true&#x27;: t = threading.Thread(target=main_func, args=args, kwargs=kwargs) t.setDaemon(True) t.start() reloader.run() else: sys.exit(reloader.restart_with_reloader()) except KeyboardInterrupt: pass 然后就能传参了 12345678910111213import timedef main(name, age): while True: print(&quot;hello, &quot;, name, &#x27;!&#x27;) time.sleep(2) print(age)if __name__ == &#x27;__main__&#x27;: run_with_reloader(main, args=(&#x27;foo&#x27;, 20))","categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"}]}],"categories":[{"name":"笔记","slug":"笔记","permalink":"https://fuyunliu.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"LSTM","slug":"LSTM","permalink":"https://fuyunliu.github.io/tags/LSTM/"},{"name":"Drain3","slug":"Drain3","permalink":"https://fuyunliu.github.io/tags/Drain3/"},{"name":"FastText","slug":"FastText","permalink":"https://fuyunliu.github.io/tags/FastText/"},{"name":"HanLP","slug":"HanLP","permalink":"https://fuyunliu.github.io/tags/HanLP/"},{"name":"JWT","slug":"JWT","permalink":"https://fuyunliu.github.io/tags/JWT/"},{"name":"Session","slug":"Session","permalink":"https://fuyunliu.github.io/tags/Session/"},{"name":"Cookie","slug":"Cookie","permalink":"https://fuyunliu.github.io/tags/Cookie/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://fuyunliu.github.io/tags/PostgreSQL/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://fuyunliu.github.io/tags/Algorithm/"},{"name":"CentOS","slug":"CentOS","permalink":"https://fuyunliu.github.io/tags/CentOS/"},{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://fuyunliu.github.io/tags/Sqlalchemy/"},{"name":"Python3","slug":"Python3","permalink":"https://fuyunliu.github.io/tags/Python3/"},{"name":"小说","slug":"小说","permalink":"https://fuyunliu.github.io/tags/%E5%B0%8F%E8%AF%B4/"},{"name":"Flask","slug":"Flask","permalink":"https://fuyunliu.github.io/tags/Flask/"},{"name":"MySQL","slug":"MySQL","permalink":"https://fuyunliu.github.io/tags/MySQL/"}]}